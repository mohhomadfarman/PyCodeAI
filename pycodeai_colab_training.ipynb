{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk_Vfist5oAW"
      },
      "source": [
        "# PyCodeAI - Google Colab Training (GitHub Version)\n",
        "\n",
        "This notebook trains your PyCodeAI model using code from GitHub and saves the results to Google Drive.\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1.  **Configure**: Set your GitHub Repository URL in the first code cell.\n",
        "2.  **Mount Drive**: Run the cell to connect Google Drive (for saving the trained model).\n",
        "3.  **Run All**: Run all cells to clone, install, and train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6Cy-ANoS5oAW"
      },
      "outputs": [],
      "source": [
        "# CONFIGURATION\n",
        "# Replace this with your repository URL\n",
        "GITHUB_REPO = 'https://github.com/mohhomadfarman/PyCodeAI.git'\n",
        "BRANCH = 'main'  # or 'master'\n",
        "\n",
        "# This is where the model will be SAVED in your Google Drive\n",
        "DRIVE_SAVE_PATH = '/content/drive/MyDrive/PyCodeAI_Models'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUmVK-Wn5oAX",
        "outputId": "c704b974-4eb8-47c4-c3c3-0785ee406bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Models will be saved to: /content/drive/MyDrive/PyCodeAI_Models\n"
          ]
        }
      ],
      "source": [
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create the save directory if it doesn't exist\n",
        "os.makedirs(DRIVE_SAVE_PATH, exist_ok=True)\n",
        "print(f\"Models will be saved to: {DRIVE_SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TGNqe4F5oAX",
        "outputId": "a64e6d9e-cf05-4e52-a4db-84d1eb58c117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PyCodeAI_Repo'...\n",
            "remote: Enumerating objects: 3039, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 3039 (delta 7), reused 7 (delta 4), pack-reused 3009 (from 4)\u001b[K\n",
            "Receiving objects: 100% (3039/3039), 41.54 MiB | 29.48 MiB/s, done.\n",
            "Resolving deltas: 100% (1063/1063), done.\n",
            "/content/PyCodeAI_Repo\n",
            "Already on 'main'\n",
            "Your branch is up to date with 'origin/main'.\n",
            "From https://github.com/mohhomadfarman/PyCodeAI\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (13.6.0)\n",
            "Requirement already satisfied: numpy<2.6,>=1.22 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x) (2.0.2)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x) (0.8.3)\n"
          ]
        }
      ],
      "source": [
        "# 2. Clone Repository & Install Dependencies\n",
        "!git clone {GITHUB_REPO} PyCodeAI_Repo\n",
        "%cd PyCodeAI_Repo\n",
        "!git checkout {BRANCH}\n",
        "!git pull origin {BRANCH}  # Ensure we have the latest\n",
        "\n",
        "# Install cupy for GPU\n",
        "!pip install cupy-cuda12x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U_ohlOhS5oAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea32d06-7c80-4973-9218-eb9c8c2f273d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training from existing 'best_model.npz'...\n"
          ]
        }
      ],
      "source": [
        "# 3. Check for Existing Model\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# If you have a 'best_model.npz' in your Drive, we can copy it here to resume training\n",
        "# Uncomment the lines below if you want to pull a model FROM Drive\n",
        "# DRIVE_MODEL = os.path.join(DRIVE_SAVE_PATH, 'best_model.npz')\n",
        "# if os.path.exists(DRIVE_MODEL):\n",
        "#     print(\"Found model in Drive, copying to local workspace...\")\n",
        "#     shutil.copy(DRIVE_MODEL, 'best_model.npz')\n",
        "\n",
        "if os.path.exists('best_model.npz'):\n",
        "    print(\"Starting training from existing 'best_model.npz'...\")\n",
        "else:\n",
        "    print(\"No 'best_model.npz' found. Starting fresh training (or finding it in repo).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Eo8fS4EV5oAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cda3f1a-27f7-479b-fda0-e264b6083e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backend: GPU (Unknown GPU, 15095MB VRAM)\n",
            "============================================================\n",
            ">> Training PyCodeAI\n",
            "Device: GPU\n",
            "============================================================\n",
            "\n",
            "1. Loading training data...\n",
            "[OK] Loaded 2062 legacy crawled files\n",
            "[OK] Loaded 1531 structured crawled files\n",
            "[OK] Loaded 103 articles\n",
            "   Loaded 8060 samples\n",
            "\n",
            "2. Building tokenizer...\n",
            "   Loading tokenizer from tokenizer.json...\n",
            "Tokenizer loaded from tokenizer.json\n",
            "   Vocabulary size: 5000\n",
            "\n",
            "3. Tokenizing data...\n",
            "   Tokenized 8060 samples\n",
            "\n",
            "4. Creating model...\n",
            "   Loading weights from best_model.npz...\n",
            "Model loaded from best_model.npz\n",
            "   [OK] Weights loaded successfully!\n",
            "   Model parameters: 2,073,344\n",
            "GPTConfig(\n",
            "  vocab_size=5000,\n",
            "  max_seq_len=64,\n",
            "  embed_dim=128,\n",
            "  num_heads=4,\n",
            "  num_layers=4,\n",
            "  expansion_factor=4\n",
            ")\n",
            "\n",
            "5. Creating data loader...\n",
            "   Batches per epoch: 2440\n",
            "\n",
            "6. Setting up trainer...\n",
            "\n",
            "7. Starting training...\n",
            "============================================================\n",
            "Starting Training\n",
            "Model parameters: 2,073,344\n",
            "Epochs: 5\n",
            "Batches per epoch: 2440\n",
            "Gradient Accumulation: 1 steps\n",
            "Effective Batch Size: 32\n",
            "============================================================\n",
            "Epoch 1/5 | Step 10 | Loss: 5.4494 | LR: 3.00e-05 | Tokens/s: 3225\n",
            "Epoch 1/5 | Step 20 | Loss: 5.3378 | LR: 6.00e-05 | Tokens/s: 5886\n",
            "Epoch 1/5 | Step 30 | Loss: 5.0665 | LR: 9.00e-05 | Tokens/s: 8135\n",
            "Epoch 1/5 | Step 40 | Loss: 5.0437 | LR: 1.20e-04 | Tokens/s: 10061\n",
            "Epoch 1/5 | Step 50 | Loss: 3.4810 | LR: 1.50e-04 | Tokens/s: 11698\n",
            "Epoch 1/5 | Step 60 | Loss: 3.2148 | LR: 1.80e-04 | Tokens/s: 13166\n",
            "Epoch 1/5 | Step 70 | Loss: 3.1339 | LR: 2.10e-04 | Tokens/s: 14371\n",
            "Epoch 1/5 | Step 80 | Loss: 3.0225 | LR: 2.40e-04 | Tokens/s: 15482\n",
            "Epoch 1/5 | Step 90 | Loss: 2.9938 | LR: 2.70e-04 | Tokens/s: 16526\n",
            "Epoch 1/5 | Step 100 | Loss: 3.2735 | LR: 3.00e-04 | Tokens/s: 17440\n",
            "Model saved to best_model.npz\n",
            "Config saved to best_model_config.json\n",
            "Checkpoint saved: best_model.npz\n",
            "Epoch 1/5 | Step 110 | Loss: 3.3024 | LR: 3.00e-04 | Tokens/s: 18248\n",
            "Epoch 1/5 | Step 120 | Loss: 2.8782 | LR: 3.00e-04 | Tokens/s: 18999\n",
            "Epoch 1/5 | Step 130 | Loss: 3.3296 | LR: 3.00e-04 | Tokens/s: 19330\n",
            "Epoch 1/5 | Step 140 | Loss: 3.6978 | LR: 3.00e-04 | Tokens/s: 19667\n",
            "Epoch 1/5 | Step 150 | Loss: 3.1971 | LR: 3.00e-04 | Tokens/s: 19980\n",
            "Epoch 1/5 | Step 160 | Loss: 3.0572 | LR: 3.00e-04 | Tokens/s: 20219\n",
            "Epoch 1/5 | Step 170 | Loss: 3.2213 | LR: 3.00e-04 | Tokens/s: 20741\n",
            "Epoch 1/5 | Step 180 | Loss: 2.9554 | LR: 3.00e-04 | Tokens/s: 21165\n",
            "Epoch 1/5 | Step 190 | Loss: 3.2730 | LR: 3.00e-04 | Tokens/s: 21558\n",
            "Epoch 1/5 | Step 200 | Loss: 4.1226 | LR: 3.00e-04 | Tokens/s: 21997\n",
            "Epoch 1/5 | Step 210 | Loss: 4.1187 | LR: 3.00e-04 | Tokens/s: 22374\n",
            "Epoch 1/5 | Step 220 | Loss: 3.1416 | LR: 3.00e-04 | Tokens/s: 22747\n",
            "Epoch 1/5 | Step 230 | Loss: 3.3275 | LR: 3.00e-04 | Tokens/s: 23080\n",
            "Epoch 1/5 | Step 240 | Loss: 3.2994 | LR: 3.00e-04 | Tokens/s: 23350\n",
            "Epoch 1/5 | Step 250 | Loss: 3.0207 | LR: 3.00e-04 | Tokens/s: 23680\n",
            "Epoch 1/5 | Step 260 | Loss: 3.1408 | LR: 3.00e-04 | Tokens/s: 23963\n",
            "Epoch 1/5 | Step 270 | Loss: 2.9339 | LR: 3.00e-04 | Tokens/s: 24245\n",
            "Epoch 1/5 | Step 280 | Loss: 3.0752 | LR: 3.00e-04 | Tokens/s: 24501\n",
            "Epoch 1/5 | Step 290 | Loss: 2.9815 | LR: 3.00e-04 | Tokens/s: 24766\n",
            "Epoch 1/5 | Step 300 | Loss: 2.9998 | LR: 3.00e-04 | Tokens/s: 25007\n",
            "Model saved to best_model.npz\n",
            "Config saved to best_model_config.json\n",
            "Checkpoint saved: best_model.npz\n",
            "Epoch 1/5 | Step 310 | Loss: 3.0847 | LR: 3.00e-04 | Tokens/s: 25171\n",
            "Epoch 1/5 | Step 320 | Loss: 3.0600 | LR: 3.00e-04 | Tokens/s: 25402\n",
            "Epoch 1/5 | Step 330 | Loss: 2.7349 | LR: 3.00e-04 | Tokens/s: 25464\n",
            "Epoch 1/5 | Step 340 | Loss: 3.0716 | LR: 3.00e-04 | Tokens/s: 25439\n",
            "Epoch 1/5 | Step 350 | Loss: 2.7159 | LR: 3.00e-04 | Tokens/s: 25424\n",
            "Epoch 1/5 | Step 360 | Loss: 2.9423 | LR: 3.00e-04 | Tokens/s: 25375\n",
            "Epoch 1/5 | Step 370 | Loss: 2.9877 | LR: 3.00e-04 | Tokens/s: 25514\n",
            "Epoch 1/5 | Step 380 | Loss: 2.6536 | LR: 3.00e-04 | Tokens/s: 25708\n",
            "Epoch 1/5 | Step 390 | Loss: 2.7935 | LR: 3.00e-04 | Tokens/s: 25870\n",
            "Epoch 1/5 | Step 400 | Loss: 2.7884 | LR: 3.00e-04 | Tokens/s: 26048\n",
            "Model saved to best_model.npz\n",
            "Config saved to best_model_config.json\n",
            "Checkpoint saved: best_model.npz\n",
            "Epoch 1/5 | Step 410 | Loss: 2.7267 | LR: 3.00e-04 | Tokens/s: 26142\n",
            "Epoch 1/5 | Step 420 | Loss: 2.7060 | LR: 2.99e-04 | Tokens/s: 26290\n",
            "Epoch 1/5 | Step 430 | Loss: 3.0998 | LR: 2.99e-04 | Tokens/s: 26395\n",
            "Epoch 1/5 | Step 440 | Loss: 2.8831 | LR: 2.99e-04 | Tokens/s: 26538\n",
            "Epoch 1/5 | Step 450 | Loss: 2.6961 | LR: 2.99e-04 | Tokens/s: 26686\n",
            "Epoch 1/5 | Step 460 | Loss: 2.5141 | LR: 2.99e-04 | Tokens/s: 26820\n",
            "Epoch 1/5 | Step 470 | Loss: 3.9299 | LR: 2.99e-04 | Tokens/s: 26954\n",
            "Epoch 1/5 | Step 480 | Loss: 3.4326 | LR: 2.99e-04 | Tokens/s: 27090\n",
            "Epoch 1/5 | Step 490 | Loss: 2.9339 | LR: 2.99e-04 | Tokens/s: 27192\n",
            "Epoch 1/5 | Step 500 | Loss: 3.3814 | LR: 2.99e-04 | Tokens/s: 27322\n",
            "Epoch 1/5 | Step 510 | Loss: 3.3305 | LR: 2.99e-04 | Tokens/s: 27428\n",
            "Epoch 1/5 | Step 520 | Loss: 3.2918 | LR: 2.99e-04 | Tokens/s: 27538\n",
            "Epoch 1/5 | Step 530 | Loss: 3.1229 | LR: 2.99e-04 | Tokens/s: 27578\n",
            "Epoch 1/5 | Step 540 | Loss: 2.9051 | LR: 2.99e-04 | Tokens/s: 27533\n",
            "Epoch 1/5 | Step 550 | Loss: 3.2479 | LR: 2.99e-04 | Tokens/s: 27440\n",
            "Epoch 1/5 | Step 560 | Loss: 3.0297 | LR: 2.99e-04 | Tokens/s: 27408\n",
            "Epoch 1/5 | Step 570 | Loss: 4.7173 | LR: 2.99e-04 | Tokens/s: 27399\n",
            "Epoch 1/5 | Step 580 | Loss: 3.5992 | LR: 2.99e-04 | Tokens/s: 27498\n",
            "Epoch 1/5 | Step 590 | Loss: 2.5841 | LR: 2.99e-04 | Tokens/s: 27604\n",
            "Epoch 1/5 | Step 600 | Loss: 3.1788 | LR: 2.99e-04 | Tokens/s: 27686\n",
            "Epoch 1/5 | Step 610 | Loss: 3.2481 | LR: 2.99e-04 | Tokens/s: 27786\n",
            "Epoch 1/5 | Step 620 | Loss: 3.2042 | LR: 2.99e-04 | Tokens/s: 27873\n",
            "Epoch 1/5 | Step 630 | Loss: 2.8728 | LR: 2.99e-04 | Tokens/s: 27960\n",
            "Epoch 1/5 | Step 640 | Loss: 3.0824 | LR: 2.99e-04 | Tokens/s: 28052\n",
            "Epoch 1/5 | Step 650 | Loss: 3.4541 | LR: 2.98e-04 | Tokens/s: 28121\n",
            "Epoch 1/5 | Step 660 | Loss: 2.9894 | LR: 2.98e-04 | Tokens/s: 28209\n",
            "Epoch 1/5 | Step 670 | Loss: 3.7414 | LR: 2.98e-04 | Tokens/s: 28260\n",
            "Epoch 1/5 | Step 680 | Loss: 3.3469 | LR: 2.98e-04 | Tokens/s: 28335\n",
            "Epoch 1/5 | Step 690 | Loss: 3.1901 | LR: 2.98e-04 | Tokens/s: 28403\n",
            "Epoch 1/5 | Step 700 | Loss: 3.0710 | LR: 2.98e-04 | Tokens/s: 28472\n",
            "Epoch 1/5 | Step 710 | Loss: 2.9996 | LR: 2.98e-04 | Tokens/s: 28556\n",
            "Epoch 1/5 | Step 720 | Loss: 3.2681 | LR: 2.98e-04 | Tokens/s: 28631\n",
            "Epoch 1/5 | Step 730 | Loss: 3.0999 | LR: 2.98e-04 | Tokens/s: 28704\n",
            "Epoch 1/5 | Step 740 | Loss: 3.4038 | LR: 2.98e-04 | Tokens/s: 28675\n",
            "Epoch 1/5 | Step 750 | Loss: 3.0048 | LR: 2.98e-04 | Tokens/s: 28618\n",
            "Epoch 1/5 | Step 760 | Loss: 2.6493 | LR: 2.98e-04 | Tokens/s: 28578\n",
            "Epoch 1/5 | Step 770 | Loss: 3.1105 | LR: 2.98e-04 | Tokens/s: 28500\n",
            "Epoch 1/5 | Step 780 | Loss: 2.3774 | LR: 2.98e-04 | Tokens/s: 28557\n",
            "Epoch 1/5 | Step 790 | Loss: 3.2831 | LR: 2.98e-04 | Tokens/s: 28597\n",
            "Epoch 1/5 | Step 800 | Loss: 2.6333 | LR: 2.98e-04 | Tokens/s: 28670\n",
            "Model saved to best_model.npz\n",
            "Config saved to best_model_config.json\n",
            "Checkpoint saved: best_model.npz\n",
            "Epoch 1/5 | Step 810 | Loss: 2.6624 | LR: 2.97e-04 | Tokens/s: 28724\n",
            "Epoch 1/5 | Step 820 | Loss: 2.9922 | LR: 2.97e-04 | Tokens/s: 28785\n",
            "Epoch 1/5 | Step 830 | Loss: 2.9544 | LR: 2.97e-04 | Tokens/s: 28849\n",
            "Epoch 1/5 | Step 840 | Loss: 2.7818 | LR: 2.97e-04 | Tokens/s: 28912\n",
            "Epoch 1/5 | Step 850 | Loss: 3.1241 | LR: 2.97e-04 | Tokens/s: 28962\n",
            "Epoch 1/5 | Step 860 | Loss: 2.9746 | LR: 2.97e-04 | Tokens/s: 29019\n",
            "Epoch 1/5 | Step 870 | Loss: 2.9225 | LR: 2.97e-04 | Tokens/s: 29077\n",
            "Epoch 1/5 | Step 880 | Loss: 2.8670 | LR: 2.97e-04 | Tokens/s: 29120\n",
            "Epoch 1/5 | Step 890 | Loss: 2.8286 | LR: 2.97e-04 | Tokens/s: 29161\n",
            "Epoch 1/5 | Step 900 | Loss: 2.6398 | LR: 2.97e-04 | Tokens/s: 29211\n",
            "Epoch 1/5 | Step 910 | Loss: 2.6955 | LR: 2.97e-04 | Tokens/s: 29245\n",
            "Epoch 1/5 | Step 920 | Loss: 2.5039 | LR: 2.97e-04 | Tokens/s: 29291\n",
            "Epoch 1/5 | Step 930 | Loss: 2.9475 | LR: 2.97e-04 | Tokens/s: 29340\n",
            "Epoch 1/5 | Step 940 | Loss: 2.9368 | LR: 2.96e-04 | Tokens/s: 29355\n",
            "Epoch 1/5 | Step 950 | Loss: 2.5333 | LR: 2.96e-04 | Tokens/s: 29313\n",
            "Epoch 1/5 | Step 960 | Loss: 3.2343 | LR: 2.96e-04 | Tokens/s: 29246\n",
            "Epoch 1/5 | Step 970 | Loss: 3.5426 | LR: 2.96e-04 | Tokens/s: 29196\n",
            "Epoch 1/5 | Step 980 | Loss: 3.2493 | LR: 2.96e-04 | Tokens/s: 29172\n",
            "Epoch 1/5 | Step 990 | Loss: 3.2601 | LR: 2.96e-04 | Tokens/s: 29205\n",
            "Epoch 1/5 | Step 1000 | Loss: 2.8329 | LR: 2.96e-04 | Tokens/s: 29250\n",
            "Epoch 1/5 | Step 1010 | Loss: 3.2936 | LR: 2.96e-04 | Tokens/s: 29292\n",
            "Epoch 1/5 | Step 1020 | Loss: 3.0938 | LR: 2.96e-04 | Tokens/s: 29337\n",
            "Epoch 1/5 | Step 1030 | Loss: 3.1810 | LR: 2.96e-04 | Tokens/s: 29381\n",
            "Epoch 1/5 | Step 1040 | Loss: 2.6872 | LR: 2.96e-04 | Tokens/s: 29399\n",
            "Epoch 1/5 | Step 1050 | Loss: 3.2044 | LR: 2.95e-04 | Tokens/s: 29451\n",
            "Epoch 1/5 | Step 1060 | Loss: 3.2640 | LR: 2.95e-04 | Tokens/s: 29496\n",
            "Epoch 1/5 | Step 1070 | Loss: 3.2228 | LR: 2.95e-04 | Tokens/s: 29543\n",
            "Epoch 1/5 | Step 1080 | Loss: 2.9703 | LR: 2.95e-04 | Tokens/s: 29586\n",
            "Epoch 1/5 | Step 1090 | Loss: 2.6790 | LR: 2.95e-04 | Tokens/s: 29625\n",
            "Epoch 1/5 | Step 1100 | Loss: 2.6862 | LR: 2.95e-04 | Tokens/s: 29675\n",
            "Epoch 1/5 | Step 1110 | Loss: 2.4441 | LR: 2.95e-04 | Tokens/s: 29715\n",
            "Epoch 1/5 | Step 1120 | Loss: 2.7567 | LR: 2.95e-04 | Tokens/s: 29757\n",
            "Epoch 1/5 | Step 1130 | Loss: 2.6642 | LR: 2.95e-04 | Tokens/s: 29797\n",
            "Epoch 1/5 | Step 1140 | Loss: 2.9111 | LR: 2.95e-04 | Tokens/s: 29836\n",
            "Epoch 1/5 | Step 1150 | Loss: 2.8077 | LR: 2.94e-04 | Tokens/s: 29809\n",
            "Epoch 1/5 | Step 1160 | Loss: 2.5945 | LR: 2.94e-04 | Tokens/s: 29726\n",
            "Epoch 1/5 | Step 1170 | Loss: 2.5817 | LR: 2.94e-04 | Tokens/s: 29673\n",
            "Epoch 1/5 | Step 1180 | Loss: 2.6766 | LR: 2.94e-04 | Tokens/s: 29612\n",
            "Epoch 1/5 | Step 1190 | Loss: 2.5344 | LR: 2.94e-04 | Tokens/s: 29625\n",
            "Epoch 1/5 | Step 1200 | Loss: 2.6553 | LR: 2.94e-04 | Tokens/s: 29660\n",
            "Epoch 1/5 | Step 1210 | Loss: 2.6400 | LR: 2.94e-04 | Tokens/s: 29701\n",
            "Epoch 1/5 | Step 1220 | Loss: 3.1507 | LR: 2.94e-04 | Tokens/s: 29737\n",
            "Epoch 1/5 | Step 1230 | Loss: 2.7301 | LR: 2.94e-04 | Tokens/s: 29777\n",
            "Epoch 1/5 | Step 1240 | Loss: 2.5137 | LR: 2.93e-04 | Tokens/s: 29812\n",
            "Epoch 1/5 | Step 1250 | Loss: 2.4299 | LR: 2.93e-04 | Tokens/s: 29848\n",
            "Epoch 1/5 | Step 1260 | Loss: 2.6288 | LR: 2.93e-04 | Tokens/s: 29886\n",
            "Epoch 1/5 | Step 1270 | Loss: 2.7599 | LR: 2.93e-04 | Tokens/s: 29918\n",
            "Epoch 1/5 | Step 1280 | Loss: 2.5199 | LR: 2.93e-04 | Tokens/s: 29941\n",
            "Epoch 1/5 | Step 1290 | Loss: 2.5048 | LR: 2.93e-04 | Tokens/s: 29971\n",
            "Epoch 1/5 | Step 1300 | Loss: 2.5549 | LR: 2.93e-04 | Tokens/s: 30010\n",
            "Model saved to best_model.npz\n",
            "Config saved to best_model_config.json\n",
            "Checkpoint saved: best_model.npz\n",
            "Epoch 1/5 | Step 1310 | Loss: 2.4989 | LR: 2.93e-04 | Tokens/s: 30031\n",
            "Epoch 1/5 | Step 1320 | Loss: 2.5235 | LR: 2.93e-04 | Tokens/s: 30059\n",
            "Epoch 1/5 | Step 1330 | Loss: 2.3179 | LR: 2.92e-04 | Tokens/s: 30095\n",
            "Epoch 1/5 | Step 1340 | Loss: 2.2908 | LR: 2.92e-04 | Tokens/s: 30108\n",
            "Epoch 1/5 | Step 1350 | Loss: 2.4334 | LR: 2.92e-04 | Tokens/s: 30138\n",
            "Epoch 1/5 | Step 1360 | Loss: 2.4304 | LR: 2.92e-04 | Tokens/s: 30089\n",
            "Epoch 1/5 | Step 1370 | Loss: 2.5442 | LR: 2.92e-04 | Tokens/s: 30043\n",
            "Epoch 1/5 | Step 1380 | Loss: 2.4305 | LR: 2.92e-04 | Tokens/s: 29999\n",
            "Epoch 1/5 | Step 1390 | Loss: 2.4672 | LR: 2.92e-04 | Tokens/s: 29965\n",
            "Epoch 1/5 | Step 1400 | Loss: 2.5500 | LR: 2.92e-04 | Tokens/s: 29978\n",
            "Model saved to best_model.npz\n",
            "Config saved to best_model_config.json\n",
            "Checkpoint saved: best_model.npz\n",
            "Epoch 1/5 | Step 1410 | Loss: 2.3446 | LR: 2.91e-04 | Tokens/s: 30001\n",
            "Epoch 1/5 | Step 1420 | Loss: 3.6265 | LR: 2.91e-04 | Tokens/s: 30035\n",
            "Epoch 1/5 | Step 1430 | Loss: 2.9872 | LR: 2.91e-04 | Tokens/s: 30065\n",
            "Epoch 1/5 | Step 1440 | Loss: 3.1474 | LR: 2.91e-04 | Tokens/s: 30099\n",
            "Epoch 1/5 | Step 1450 | Loss: 2.9621 | LR: 2.91e-04 | Tokens/s: 30124\n",
            "Epoch 1/5 | Step 1460 | Loss: 2.9188 | LR: 2.91e-04 | Tokens/s: 30149\n",
            "Epoch 1/5 | Step 1470 | Loss: 2.9976 | LR: 2.91e-04 | Tokens/s: 30176\n",
            "Epoch 1/5 | Step 1480 | Loss: 2.8567 | LR: 2.91e-04 | Tokens/s: 30202\n",
            "Epoch 1/5 | Step 1490 | Loss: 2.7864 | LR: 2.90e-04 | Tokens/s: 30235\n",
            "Epoch 1/5 | Step 1500 | Loss: 2.7913 | LR: 2.90e-04 | Tokens/s: 30258\n",
            "Epoch 1/5 | Step 1510 | Loss: 2.6228 | LR: 2.90e-04 | Tokens/s: 30287\n",
            "Epoch 1/5 | Step 1520 | Loss: 2.8196 | LR: 2.90e-04 | Tokens/s: 30302\n",
            "Epoch 1/5 | Step 1530 | Loss: 2.7912 | LR: 2.90e-04 | Tokens/s: 30330\n",
            "Epoch 1/5 | Step 1540 | Loss: 2.4655 | LR: 2.90e-04 | Tokens/s: 30354\n",
            "Epoch 1/5 | Step 1550 | Loss: 2.7657 | LR: 2.90e-04 | Tokens/s: 30378\n",
            "Epoch 1/5 | Step 1560 | Loss: 3.1392 | LR: 2.89e-04 | Tokens/s: 30366\n",
            "Epoch 1/5 | Step 1570 | Loss: 2.8709 | LR: 2.89e-04 | Tokens/s: 30330\n",
            "Epoch 1/5 | Step 1580 | Loss: 3.0786 | LR: 2.89e-04 | Tokens/s: 30284\n",
            "Epoch 1/5 | Step 1590 | Loss: 2.7011 | LR: 2.89e-04 | Tokens/s: 30232\n",
            "Epoch 1/5 | Step 1600 | Loss: 2.8029 | LR: 2.89e-04 | Tokens/s: 30239\n",
            "Epoch 1/5 | Step 1610 | Loss: 2.6816 | LR: 2.89e-04 | Tokens/s: 30262\n",
            "Epoch 1/5 | Step 1620 | Loss: 2.7125 | LR: 2.89e-04 | Tokens/s: 30289\n",
            "Epoch 1/5 | Step 1630 | Loss: 2.3771 | LR: 2.88e-04 | Tokens/s: 30310\n",
            "Epoch 1/5 | Step 1640 | Loss: 2.2717 | LR: 2.88e-04 | Tokens/s: 30323\n",
            "Epoch 1/5 | Step 1650 | Loss: 2.2131 | LR: 2.88e-04 | Tokens/s: 30347\n",
            "Epoch 1/5 | Step 1660 | Loss: 1.9743 | LR: 2.88e-04 | Tokens/s: 30369\n",
            "Epoch 1/5 | Step 1670 | Loss: 3.2527 | LR: 2.88e-04 | Tokens/s: 30391\n",
            "Epoch 1/5 | Step 1680 | Loss: 3.4746 | LR: 2.88e-04 | Tokens/s: 30414\n",
            "Epoch 1/5 | Step 1690 | Loss: 2.9150 | LR: 2.87e-04 | Tokens/s: 30440\n",
            "Epoch 1/5 | Step 1700 | Loss: 2.9703 | LR: 2.87e-04 | Tokens/s: 30461\n",
            "Epoch 1/5 | Step 1710 | Loss: 2.7902 | LR: 2.87e-04 | Tokens/s: 30487\n",
            "Epoch 1/5 | Step 1720 | Loss: 2.7003 | LR: 2.87e-04 | Tokens/s: 30509\n",
            "Epoch 1/5 | Step 1730 | Loss: 3.1617 | LR: 2.87e-04 | Tokens/s: 30529\n",
            "Epoch 1/5 | Step 1740 | Loss: 3.5207 | LR: 2.87e-04 | Tokens/s: 30554\n",
            "Epoch 1/5 | Step 1750 | Loss: 3.1781 | LR: 2.86e-04 | Tokens/s: 30575\n",
            "Epoch 1/5 | Step 1760 | Loss: 2.8928 | LR: 2.86e-04 | Tokens/s: 30586\n",
            "Epoch 1/5 | Step 1770 | Loss: 2.4448 | LR: 2.86e-04 | Tokens/s: 30544\n",
            "Epoch 1/5 | Step 1780 | Loss: 2.6880 | LR: 2.86e-04 | Tokens/s: 30502\n",
            "Epoch 1/5 | Step 1790 | Loss: 2.0806 | LR: 2.86e-04 | Tokens/s: 30474\n",
            "Epoch 1/5 | Step 1800 | Loss: 2.8683 | LR: 2.86e-04 | Tokens/s: 30433\n",
            "Epoch 1/5 | Step 1810 | Loss: 2.1186 | LR: 2.86e-04 | Tokens/s: 30453\n",
            "Epoch 1/5 | Step 1820 | Loss: 2.6938 | LR: 2.85e-04 | Tokens/s: 30469\n",
            "Epoch 1/5 | Step 1830 | Loss: 2.2047 | LR: 2.85e-04 | Tokens/s: 30493\n",
            "Epoch 1/5 | Step 1840 | Loss: 3.1660 | LR: 2.85e-04 | Tokens/s: 30513\n",
            "Epoch 1/5 | Step 1850 | Loss: 2.2598 | LR: 2.85e-04 | Tokens/s: 30536\n",
            "Epoch 1/5 | Step 1860 | Loss: 2.5863 | LR: 2.85e-04 | Tokens/s: 30557\n",
            "Epoch 1/5 | Step 1870 | Loss: 3.5131 | LR: 2.84e-04 | Tokens/s: 30579\n",
            "Epoch 1/5 | Step 1880 | Loss: 2.4629 | LR: 2.84e-04 | Tokens/s: 30586\n",
            "Epoch 1/5 | Step 1890 | Loss: 2.7182 | LR: 2.84e-04 | Tokens/s: 30604\n",
            "Epoch 1/5 | Step 1900 | Loss: 2.6784 | LR: 2.84e-04 | Tokens/s: 30626\n",
            "Epoch 1/5 | Step 1910 | Loss: 2.4475 | LR: 2.84e-04 | Tokens/s: 30644\n",
            "Epoch 1/5 | Step 1920 | Loss: 2.9758 | LR: 2.84e-04 | Tokens/s: 30665\n",
            "Epoch 1/5 | Step 1930 | Loss: 3.5153 | LR: 2.83e-04 | Tokens/s: 30681\n",
            "Epoch 1/5 | Step 1940 | Loss: 2.1068 | LR: 2.83e-04 | Tokens/s: 30697\n",
            "Epoch 1/5 | Step 1950 | Loss: 3.0718 | LR: 2.83e-04 | Tokens/s: 30716\n",
            "Epoch 1/5 | Step 1960 | Loss: 2.2970 | LR: 2.83e-04 | Tokens/s: 30732\n",
            "Epoch 1/5 | Step 1970 | Loss: 2.0845 | LR: 2.83e-04 | Tokens/s: 30728\n",
            "Epoch 1/5 | Step 1980 | Loss: 3.0298 | LR: 2.83e-04 | Tokens/s: 30697\n",
            "Epoch 1/5 | Step 1990 | Loss: 2.5365 | LR: 2.82e-04 | Tokens/s: 30659\n",
            "Epoch 1/5 | Step 2000 | Loss: 2.7675 | LR: 2.82e-04 | Tokens/s: 30605\n",
            "Epoch 1/5 | Step 2010 | Loss: 2.0843 | LR: 2.82e-04 | Tokens/s: 30599\n",
            "Epoch 1/5 | Step 2020 | Loss: 2.5229 | LR: 2.82e-04 | Tokens/s: 30617\n",
            "Epoch 1/5 | Step 2030 | Loss: 2.3844 | LR: 2.82e-04 | Tokens/s: 30638\n",
            "Epoch 1/5 | Step 2040 | Loss: 2.1518 | LR: 2.81e-04 | Tokens/s: 30653\n",
            "Epoch 1/5 | Step 2050 | Loss: 2.7740 | LR: 2.81e-04 | Tokens/s: 30670\n",
            "Epoch 1/5 | Step 2060 | Loss: 2.9035 | LR: 2.81e-04 | Tokens/s: 30688\n",
            "Epoch 1/5 | Step 2070 | Loss: 2.6419 | LR: 2.81e-04 | Tokens/s: 30704\n",
            "Epoch 1/5 | Step 2080 | Loss: 3.1208 | LR: 2.81e-04 | Tokens/s: 30724\n",
            "Epoch 1/5 | Step 2090 | Loss: 3.0887 | LR: 2.80e-04 | Tokens/s: 30741\n",
            "Epoch 1/5 | Step 2100 | Loss: 2.7495 | LR: 2.80e-04 | Tokens/s: 30757\n",
            "Epoch 1/5 | Step 2110 | Loss: 2.8860 | LR: 2.80e-04 | Tokens/s: 30772\n",
            "Epoch 1/5 | Step 2120 | Loss: 2.7067 | LR: 2.80e-04 | Tokens/s: 30779\n",
            "Epoch 1/5 | Step 2130 | Loss: 2.6462 | LR: 2.80e-04 | Tokens/s: 30798\n",
            "Epoch 1/5 | Step 2140 | Loss: 2.4439 | LR: 2.80e-04 | Tokens/s: 30814\n",
            "Epoch 1/5 | Step 2150 | Loss: 3.3684 | LR: 2.79e-04 | Tokens/s: 30831\n",
            "Epoch 1/5 | Step 2160 | Loss: 3.2460 | LR: 2.79e-04 | Tokens/s: 30845\n",
            "Epoch 1/5 | Step 2170 | Loss: 3.0820 | LR: 2.79e-04 | Tokens/s: 30860\n",
            "Epoch 1/5 | Step 2180 | Loss: 3.2350 | LR: 2.79e-04 | Tokens/s: 30829\n",
            "Epoch 1/5 | Step 2190 | Loss: 3.6071 | LR: 2.79e-04 | Tokens/s: 30793\n",
            "Epoch 1/5 | Step 2200 | Loss: 2.1790 | LR: 2.78e-04 | Tokens/s: 30768\n",
            "Model saved to best_model.npz\n",
            "Config saved to best_model_config.json\n",
            "Checkpoint saved: best_model.npz\n",
            "Epoch 1/5 | Step 2210 | Loss: 2.9967 | LR: 2.78e-04 | Tokens/s: 30717\n",
            "Epoch 1/5 | Step 2220 | Loss: 3.1541 | LR: 2.78e-04 | Tokens/s: 30735\n",
            "Epoch 1/5 | Step 2230 | Loss: 3.3163 | LR: 2.78e-04 | Tokens/s: 30749\n",
            "Epoch 1/5 | Step 2240 | Loss: 2.9505 | LR: 2.78e-04 | Tokens/s: 30764\n",
            "Epoch 1/5 | Step 2250 | Loss: 3.1011 | LR: 2.77e-04 | Tokens/s: 30767\n",
            "Epoch 1/5 | Step 2260 | Loss: 4.6294 | LR: 2.77e-04 | Tokens/s: 30784\n",
            "Epoch 1/5 | Step 2270 | Loss: 2.9140 | LR: 2.77e-04 | Tokens/s: 30792\n",
            "Epoch 1/5 | Step 2280 | Loss: 2.7740 | LR: 2.77e-04 | Tokens/s: 30801\n",
            "Epoch 1/5 | Step 2290 | Loss: 3.2177 | LR: 2.76e-04 | Tokens/s: 30820\n",
            "Epoch 1/5 | Step 2300 | Loss: 5.0013 | LR: 2.76e-04 | Tokens/s: 30833\n",
            "Epoch 1/5 | Step 2310 | Loss: 4.3301 | LR: 2.76e-04 | Tokens/s: 30851\n",
            "Epoch 1/5 | Step 2320 | Loss: 4.2471 | LR: 2.76e-04 | Tokens/s: 30865\n",
            "Epoch 1/5 | Step 2330 | Loss: 4.0313 | LR: 2.76e-04 | Tokens/s: 30877\n",
            "Epoch 1/5 | Step 2340 | Loss: 3.5469 | LR: 2.75e-04 | Tokens/s: 30892\n",
            "Epoch 1/5 | Step 2350 | Loss: 3.6873 | LR: 2.75e-04 | Tokens/s: 30902\n",
            "Epoch 1/5 | Step 2360 | Loss: 3.3978 | LR: 2.75e-04 | Tokens/s: 30920\n",
            "Epoch 1/5 | Step 2370 | Loss: 3.5119 | LR: 2.75e-04 | Tokens/s: 30927\n",
            "Epoch 1/5 | Step 2380 | Loss: 3.4129 | LR: 2.75e-04 | Tokens/s: 30917\n",
            "Epoch 1/5 | Step 2390 | Loss: 3.7359 | LR: 2.74e-04 | Tokens/s: 30891\n",
            "Epoch 1/5 | Step 2400 | Loss: 3.5196 | LR: 2.74e-04 | Tokens/s: 30860\n",
            "Epoch 1/5 | Step 2410 | Loss: 3.2998 | LR: 2.74e-04 | Tokens/s: 30827\n",
            "Epoch 1/5 | Step 2420 | Loss: 3.2288 | LR: 2.74e-04 | Tokens/s: 30821\n",
            "Epoch 1/5 | Step 2430 | Loss: 3.6798 | LR: 2.73e-04 | Tokens/s: 30836\n",
            "Epoch 1/5 | Step 2440 | Loss: 3.4096 | LR: 2.73e-04 | Tokens/s: 30847\n",
            "\n",
            "========================================\n",
            "Epoch 1 complete | Average Loss: 3.0300\n",
            "========================================\n",
            "\n",
            "Epoch 2/5 | Step 2450 | Loss: 4.4809 | LR: 2.73e-04 | Tokens/s: 30864\n",
            "Epoch 2/5 | Step 2460 | Loss: 4.0337 | LR: 2.73e-04 | Tokens/s: 30878\n",
            "Epoch 2/5 | Step 2470 | Loss: 4.1323 | LR: 2.73e-04 | Tokens/s: 30894\n",
            "Epoch 2/5 | Step 2480 | Loss: 3.8347 | LR: 2.72e-04 | Tokens/s: 30909\n",
            "Epoch 2/5 | Step 2490 | Loss: 4.0969 | LR: 2.72e-04 | Tokens/s: 30910\n",
            "Epoch 2/5 | Step 2500 | Loss: 3.6022 | LR: 2.72e-04 | Tokens/s: 30923\n",
            "Epoch 2/5 | Step 2510 | Loss: 3.3620 | LR: 2.72e-04 | Tokens/s: 30937\n",
            "Epoch 2/5 | Step 2520 | Loss: 3.0971 | LR: 2.71e-04 | Tokens/s: 30954\n",
            "Epoch 2/5 | Step 2530 | Loss: 3.2201 | LR: 2.71e-04 | Tokens/s: 30967\n",
            "Epoch 2/5 | Step 2540 | Loss: 3.2035 | LR: 2.71e-04 | Tokens/s: 30978\n",
            "Epoch 2/5 | Step 2550 | Loss: 3.1875 | LR: 2.71e-04 | Tokens/s: 30992\n",
            "Epoch 2/5 | Step 2560 | Loss: 2.8417 | LR: 2.71e-04 | Tokens/s: 31003\n",
            "Epoch 2/5 | Step 2570 | Loss: 3.2718 | LR: 2.70e-04 | Tokens/s: 31019\n",
            "Epoch 2/5 | Step 2580 | Loss: 3.7238 | LR: 2.70e-04 | Tokens/s: 31031\n",
            "Epoch 2/5 | Step 2590 | Loss: 3.1284 | LR: 2.70e-04 | Tokens/s: 31007\n",
            "Epoch 2/5 | Step 2600 | Loss: 3.1032 | LR: 2.70e-04 | Tokens/s: 30976\n",
            "Epoch 2/5 | Step 2610 | Loss: 3.0500 | LR: 2.69e-04 | Tokens/s: 30943\n",
            "Epoch 2/5 | Step 2620 | Loss: 2.8207 | LR: 2.69e-04 | Tokens/s: 30907\n",
            "Epoch 2/5 | Step 2630 | Loss: 3.2687 | LR: 2.69e-04 | Tokens/s: 30923\n",
            "Epoch 2/5 | Step 2640 | Loss: 3.1069 | LR: 2.69e-04 | Tokens/s: 30934\n",
            "Epoch 2/5 | Step 2650 | Loss: 4.0920 | LR: 2.68e-04 | Tokens/s: 30943\n",
            "Epoch 2/5 | Step 2660 | Loss: 3.0105 | LR: 2.68e-04 | Tokens/s: 30956\n",
            "Epoch 2/5 | Step 2670 | Loss: 3.2226 | LR: 2.68e-04 | Tokens/s: 30968\n",
            "Epoch 2/5 | Step 2680 | Loss: 3.2521 | LR: 2.68e-04 | Tokens/s: 30984\n",
            "Epoch 2/5 | Step 2690 | Loss: 2.9566 | LR: 2.67e-04 | Tokens/s: 30996\n",
            "Epoch 2/5 | Step 2700 | Loss: 3.0827 | LR: 2.67e-04 | Tokens/s: 31011\n",
            "Epoch 2/5 | Step 2710 | Loss: 2.8163 | LR: 2.67e-04 | Tokens/s: 31022\n",
            "Epoch 2/5 | Step 2720 | Loss: 2.9852 | LR: 2.67e-04 | Tokens/s: 31036\n",
            "Epoch 2/5 | Step 2730 | Loss: 2.8361 | LR: 2.66e-04 | Tokens/s: 31041\n",
            "Epoch 2/5 | Step 2740 | Loss: 2.8517 | LR: 2.66e-04 | Tokens/s: 31054\n",
            "Epoch 2/5 | Step 2750 | Loss: 2.9527 | LR: 2.66e-04 | Tokens/s: 31069\n",
            "Epoch 2/5 | Step 2760 | Loss: 2.9561 | LR: 2.66e-04 | Tokens/s: 31078\n",
            "Epoch 2/5 | Step 2770 | Loss: 2.6265 | LR: 2.65e-04 | Tokens/s: 31092\n",
            "Epoch 2/5 | Step 2780 | Loss: 2.9158 | LR: 2.65e-04 | Tokens/s: 31102\n",
            "Epoch 2/5 | Step 2790 | Loss: 2.5719 | LR: 2.65e-04 | Tokens/s: 31106\n",
            "Epoch 2/5 | Step 2800 | Loss: 2.8014 | LR: 2.65e-04 | Tokens/s: 31082\n",
            "Epoch 2/5 | Step 2810 | Loss: 2.9143 | LR: 2.64e-04 | Tokens/s: 31055\n",
            "Epoch 2/5 | Step 2820 | Loss: 2.5359 | LR: 2.64e-04 | Tokens/s: 31033\n",
            "Epoch 2/5 | Step 2830 | Loss: 2.7032 | LR: 2.64e-04 | Tokens/s: 31006\n",
            "Epoch 2/5 | Step 2840 | Loss: 2.6560 | LR: 2.64e-04 | Tokens/s: 31021\n",
            "Epoch 2/5 | Step 2850 | Loss: 2.5605 | LR: 2.63e-04 | Tokens/s: 31025\n",
            "Epoch 2/5 | Step 2860 | Loss: 2.5629 | LR: 2.63e-04 | Tokens/s: 31038\n",
            "Epoch 2/5 | Step 2870 | Loss: 2.9639 | LR: 2.63e-04 | Tokens/s: 31048\n",
            "Epoch 2/5 | Step 2880 | Loss: 2.7729 | LR: 2.63e-04 | Tokens/s: 31057\n",
            "Epoch 2/5 | Step 2890 | Loss: 2.5812 | LR: 2.62e-04 | Tokens/s: 31069\n",
            "Epoch 2/5 | Step 2900 | Loss: 2.5656 | LR: 2.62e-04 | Tokens/s: 31080\n",
            "Epoch 2/5 | Step 2910 | Loss: 4.0321 | LR: 2.62e-04 | Tokens/s: 31095\n",
            "Epoch 2/5 | Step 2920 | Loss: 3.4961 | LR: 2.62e-04 | Tokens/s: 31105\n",
            "Epoch 2/5 | Step 2930 | Loss: 2.7993 | LR: 2.61e-04 | Tokens/s: 31117\n",
            "Epoch 2/5 | Step 2940 | Loss: 3.2612 | LR: 2.61e-04 | Tokens/s: 31129\n",
            "Epoch 2/5 | Step 2950 | Loss: 3.1654 | LR: 2.61e-04 | Tokens/s: 31140\n",
            "Epoch 2/5 | Step 2960 | Loss: 3.1061 | LR: 2.61e-04 | Tokens/s: 31150\n",
            "Epoch 2/5 | Step 2970 | Loss: 2.8955 | LR: 2.60e-04 | Tokens/s: 31154\n",
            "Epoch 2/5 | Step 2980 | Loss: 2.7196 | LR: 2.60e-04 | Tokens/s: 31166\n",
            "Epoch 2/5 | Step 2990 | Loss: 3.1551 | LR: 2.60e-04 | Tokens/s: 31176\n",
            "Epoch 2/5 | Step 3000 | Loss: 2.8269 | LR: 2.60e-04 | Tokens/s: 31164\n",
            "Epoch 2/5 | Step 3010 | Loss: 4.8381 | LR: 2.59e-04 | Tokens/s: 31139\n",
            "Epoch 2/5 | Step 3020 | Loss: 3.6136 | LR: 2.59e-04 | Tokens/s: 31114\n",
            "Epoch 2/5 | Step 3030 | Loss: 2.5061 | LR: 2.59e-04 | Tokens/s: 31083\n",
            "Epoch 2/5 | Step 3040 | Loss: 2.9922 | LR: 2.59e-04 | Tokens/s: 31086\n",
            "Epoch 2/5 | Step 3050 | Loss: 3.0407 | LR: 2.58e-04 | Tokens/s: 31095\n",
            "Epoch 2/5 | Step 3060 | Loss: 3.1234 | LR: 2.58e-04 | Tokens/s: 31108\n",
            "Epoch 2/5 | Step 3070 | Loss: 2.7406 | LR: 2.58e-04 | Tokens/s: 31117\n",
            "Epoch 2/5 | Step 3080 | Loss: 2.9449 | LR: 2.57e-04 | Tokens/s: 31127\n",
            "Epoch 2/5 | Step 3090 | Loss: 3.3113 | LR: 2.57e-04 | Tokens/s: 31133\n",
            "Epoch 2/5 | Step 3100 | Loss: 2.8364 | LR: 2.57e-04 | Tokens/s: 31143\n",
            "Epoch 2/5 | Step 3110 | Loss: 3.4956 | LR: 2.57e-04 | Tokens/s: 31155\n",
            "Epoch 2/5 | Step 3120 | Loss: 3.2279 | LR: 2.56e-04 | Tokens/s: 31165\n",
            "Epoch 2/5 | Step 3130 | Loss: 3.0334 | LR: 2.56e-04 | Tokens/s: 31177\n",
            "Epoch 2/5 | Step 3140 | Loss: 2.9385 | LR: 2.56e-04 | Tokens/s: 31187\n",
            "Epoch 2/5 | Step 3150 | Loss: 2.8125 | LR: 2.56e-04 | Tokens/s: 31194\n",
            "Epoch 2/5 | Step 3160 | Loss: 3.0610 | LR: 2.55e-04 | Tokens/s: 31205\n",
            "Epoch 2/5 | Step 3170 | Loss: 3.0001 | LR: 2.55e-04 | Tokens/s: 31215\n",
            "Epoch 2/5 | Step 3180 | Loss: 3.3434 | LR: 2.55e-04 | Tokens/s: 31227\n",
            "Epoch 2/5 | Step 3190 | Loss: 3.0880 | LR: 2.54e-04 | Tokens/s: 31237\n",
            "Epoch 2/5 | Step 3200 | Loss: 2.8305 | LR: 2.54e-04 | Tokens/s: 31246\n",
            "Epoch 2/5 | Step 3210 | Loss: 3.2665 | LR: 2.54e-04 | Tokens/s: 31212\n",
            "Epoch 2/5 | Step 3220 | Loss: 2.5502 | LR: 2.54e-04 | Tokens/s: 31185\n",
            "Epoch 2/5 | Step 3230 | Loss: 2.9144 | LR: 2.53e-04 | Tokens/s: 31165\n",
            "Epoch 2/5 | Step 3240 | Loss: 2.4125 | LR: 2.53e-04 | Tokens/s: 31139\n",
            "Epoch 2/5 | Step 3250 | Loss: 2.5170 | LR: 2.53e-04 | Tokens/s: 31151\n",
            "Epoch 2/5 | Step 3260 | Loss: 2.8565 | LR: 2.52e-04 | Tokens/s: 31160\n",
            "Epoch 2/5 | Step 3270 | Loss: 2.9279 | LR: 2.52e-04 | Tokens/s: 31172\n",
            "Epoch 2/5 | Step 3280 | Loss: 2.7691 | LR: 2.52e-04 | Tokens/s: 31182\n",
            "Epoch 2/5 | Step 3290 | Loss: 3.0865 | LR: 2.52e-04 | Tokens/s: 31193\n",
            "Epoch 2/5 | Step 3300 | Loss: 2.8954 | LR: 2.51e-04 | Tokens/s: 31202\n",
            "Epoch 2/5 | Step 3310 | Loss: 2.7974 | LR: 2.51e-04 | Tokens/s: 31209\n",
            "Epoch 2/5 | Step 3320 | Loss: 2.7738 | LR: 2.51e-04 | Tokens/s: 31220\n",
            "Epoch 2/5 | Step 3330 | Loss: 2.7694 | LR: 2.50e-04 | Tokens/s: 31223\n",
            "Epoch 2/5 | Step 3340 | Loss: 2.5359 | LR: 2.50e-04 | Tokens/s: 31233\n",
            "Epoch 2/5 | Step 3350 | Loss: 2.6345 | LR: 2.50e-04 | Tokens/s: 31238\n",
            "Epoch 2/5 | Step 3360 | Loss: 2.4155 | LR: 2.50e-04 | Tokens/s: 31246\n",
            "Epoch 2/5 | Step 3370 | Loss: 2.8536 | LR: 2.49e-04 | Tokens/s: 31258\n",
            "Epoch 2/5 | Step 3380 | Loss: 2.8483 | LR: 2.49e-04 | Tokens/s: 31268\n",
            "Epoch 2/5 | Step 3390 | Loss: 2.3989 | LR: 2.49e-04 | Tokens/s: 31278\n",
            "Epoch 2/5 | Step 3400 | Loss: 3.2118 | LR: 2.48e-04 | Tokens/s: 31288\n",
            "Epoch 2/5 | Step 3410 | Loss: 3.6197 | LR: 2.48e-04 | Tokens/s: 31282\n",
            "Epoch 2/5 | Step 3420 | Loss: 3.2407 | LR: 2.48e-04 | Tokens/s: 31262\n",
            "Epoch 2/5 | Step 3430 | Loss: 3.3730 | LR: 2.48e-04 | Tokens/s: 31239\n",
            "Epoch 2/5 | Step 3440 | Loss: 2.7799 | LR: 2.47e-04 | Tokens/s: 31216\n",
            "Epoch 2/5 | Step 3450 | Loss: 3.3293 | LR: 2.47e-04 | Tokens/s: 31208\n",
            "Epoch 2/5 | Step 3460 | Loss: 3.1164 | LR: 2.47e-04 | Tokens/s: 31209\n",
            "Epoch 2/5 | Step 3470 | Loss: 3.2207 | LR: 2.46e-04 | Tokens/s: 31215\n",
            "Epoch 2/5 | Step 3480 | Loss: 2.5659 | LR: 2.46e-04 | Tokens/s: 31225\n",
            "Epoch 2/5 | Step 3490 | Loss: 3.0625 | LR: 2.46e-04 | Tokens/s: 31233\n",
            "Epoch 2/5 | Step 3500 | Loss: 3.1656 | LR: 2.45e-04 | Tokens/s: 31243\n",
            "Epoch 2/5 | Step 3510 | Loss: 3.1211 | LR: 2.45e-04 | Tokens/s: 31250\n",
            "Epoch 2/5 | Step 3520 | Loss: 2.8733 | LR: 2.45e-04 | Tokens/s: 31260\n",
            "Epoch 2/5 | Step 3530 | Loss: 2.6038 | LR: 2.45e-04 | Tokens/s: 31267\n",
            "Epoch 2/5 | Step 3540 | Loss: 2.6375 | LR: 2.44e-04 | Tokens/s: 31272\n",
            "Epoch 2/5 | Step 3550 | Loss: 2.3279 | LR: 2.44e-04 | Tokens/s: 31275\n",
            "Epoch 2/5 | Step 3560 | Loss: 2.6488 | LR: 2.44e-04 | Tokens/s: 31284\n",
            "Epoch 2/5 | Step 3570 | Loss: 2.5324 | LR: 2.43e-04 | Tokens/s: 31295\n",
            "Epoch 2/5 | Step 3580 | Loss: 2.8327 | LR: 2.43e-04 | Tokens/s: 31297\n",
            "Epoch 2/5 | Step 3590 | Loss: 2.7378 | LR: 2.43e-04 | Tokens/s: 31305\n",
            "Epoch 2/5 | Step 3600 | Loss: 2.4991 | LR: 2.42e-04 | Tokens/s: 31315\n",
            "Epoch 2/5 | Step 3610 | Loss: 2.5339 | LR: 2.42e-04 | Tokens/s: 31322\n",
            "Epoch 2/5 | Step 3620 | Loss: 2.5841 | LR: 2.42e-04 | Tokens/s: 31298\n",
            "Epoch 2/5 | Step 3630 | Loss: 2.4532 | LR: 2.41e-04 | Tokens/s: 31277\n",
            "Epoch 2/5 | Step 3640 | Loss: 2.6212 | LR: 2.41e-04 | Tokens/s: 31259\n",
            "Epoch 2/5 | Step 3650 | Loss: 2.5109 | LR: 2.41e-04 | Tokens/s: 31234\n",
            "Epoch 2/5 | Step 3660 | Loss: 3.0720 | LR: 2.41e-04 | Tokens/s: 31245\n",
            "Epoch 2/5 | Step 3670 | Loss: 2.6492 | LR: 2.40e-04 | Tokens/s: 31254\n",
            "Epoch 2/5 | Step 3680 | Loss: 2.4190 | LR: 2.40e-04 | Tokens/s: 31262\n",
            "Epoch 2/5 | Step 3690 | Loss: 2.3846 | LR: 2.40e-04 | Tokens/s: 31272\n",
            "Epoch 2/5 | Step 3700 | Loss: 2.5655 | LR: 2.39e-04 | Tokens/s: 31275\n",
            "Epoch 2/5 | Step 3710 | Loss: 2.7196 | LR: 2.39e-04 | Tokens/s: 31284\n",
            "Epoch 2/5 | Step 3720 | Loss: 2.5049 | LR: 2.39e-04 | Tokens/s: 31292\n",
            "Epoch 2/5 | Step 3730 | Loss: 2.5010 | LR: 2.38e-04 | Tokens/s: 31302\n",
            "Epoch 2/5 | Step 3740 | Loss: 2.5290 | LR: 2.38e-04 | Tokens/s: 31309\n",
            "Epoch 2/5 | Step 3750 | Loss: 2.4924 | LR: 2.38e-04 | Tokens/s: 31318\n",
            "Epoch 2/5 | Step 3760 | Loss: 2.6042 | LR: 2.37e-04 | Tokens/s: 31326\n",
            "Epoch 2/5 | Step 3770 | Loss: 2.4218 | LR: 2.37e-04 | Tokens/s: 31334\n",
            "Epoch 2/5 | Step 3780 | Loss: 2.2904 | LR: 2.37e-04 | Tokens/s: 31341\n",
            "Epoch 2/5 | Step 3790 | Loss: 2.4089 | LR: 2.36e-04 | Tokens/s: 31349\n",
            "Epoch 2/5 | Step 3800 | Loss: 2.4537 | LR: 2.36e-04 | Tokens/s: 31356\n",
            "Epoch 2/5 | Step 3810 | Loss: 2.5537 | LR: 2.36e-04 | Tokens/s: 31360\n",
            "Epoch 2/5 | Step 3820 | Loss: 2.3554 | LR: 2.36e-04 | Tokens/s: 31343\n",
            "Epoch 2/5 | Step 3830 | Loss: 2.4055 | LR: 2.35e-04 | Tokens/s: 31326\n",
            "Epoch 2/5 | Step 3840 | Loss: 2.5425 | LR: 2.35e-04 | Tokens/s: 31303\n",
            "Epoch 2/5 | Step 3850 | Loss: 2.3479 | LR: 2.35e-04 | Tokens/s: 31278\n",
            "Epoch 2/5 | Step 3860 | Loss: 3.4020 | LR: 2.34e-04 | Tokens/s: 31266\n",
            "Epoch 2/5 | Step 3870 | Loss: 2.9057 | LR: 2.34e-04 | Tokens/s: 31275\n",
            "Epoch 2/5 | Step 3880 | Loss: 3.1158 | LR: 2.34e-04 | Tokens/s: 31281\n",
            "Epoch 2/5 | Step 3890 | Loss: 3.0428 | LR: 2.33e-04 | Tokens/s: 31290\n",
            "Epoch 2/5 | Step 3900 | Loss: 2.8560 | LR: 2.33e-04 | Tokens/s: 31298\n",
            "Epoch 2/5 | Step 3910 | Loss: 2.9294 | LR: 2.33e-04 | Tokens/s: 31305\n",
            "Epoch 2/5 | Step 3920 | Loss: 2.9517 | LR: 2.32e-04 | Tokens/s: 31314\n",
            "Epoch 2/5 | Step 3930 | Loss: 2.7009 | LR: 2.32e-04 | Tokens/s: 31320\n",
            "Epoch 2/5 | Step 3940 | Loss: 2.7165 | LR: 2.32e-04 | Tokens/s: 31324\n",
            "Epoch 2/5 | Step 3950 | Loss: 2.5954 | LR: 2.31e-04 | Tokens/s: 31326\n",
            "Epoch 2/5 | Step 3960 | Loss: 2.7797 | LR: 2.31e-04 | Tokens/s: 31333\n",
            "Epoch 2/5 | Step 3970 | Loss: 2.8032 | LR: 2.31e-04 | Tokens/s: 31340\n",
            "Epoch 2/5 | Step 3980 | Loss: 2.4071 | LR: 2.30e-04 | Tokens/s: 31347\n",
            "Epoch 2/5 | Step 3990 | Loss: 2.6777 | LR: 2.30e-04 | Tokens/s: 31355\n",
            "Epoch 2/5 | Step 4000 | Loss: 3.0197 | LR: 2.30e-04 | Tokens/s: 31360\n",
            "Epoch 2/5 | Step 4010 | Loss: 2.7961 | LR: 2.29e-04 | Tokens/s: 31368\n",
            "Epoch 2/5 | Step 4020 | Loss: 3.0399 | LR: 2.29e-04 | Tokens/s: 31375\n",
            "Epoch 2/5 | Step 4030 | Loss: 2.5895 | LR: 2.29e-04 | Tokens/s: 31351\n",
            "Epoch 2/5 | Step 4040 | Loss: 2.7009 | LR: 2.28e-04 | Tokens/s: 31330\n",
            "Epoch 2/5 | Step 4050 | Loss: 2.6011 | LR: 2.28e-04 | Tokens/s: 31315\n",
            "Epoch 2/5 | Step 4060 | Loss: 2.7172 | LR: 2.28e-04 | Tokens/s: 31289\n",
            "Epoch 2/5 | Step 4070 | Loss: 2.3667 | LR: 2.27e-04 | Tokens/s: 31296\n",
            "Epoch 2/5 | Step 4080 | Loss: 2.3744 | LR: 2.27e-04 | Tokens/s: 31305\n",
            "Epoch 2/5 | Step 4090 | Loss: 2.3543 | LR: 2.27e-04 | Tokens/s: 31313\n",
            "Epoch 2/5 | Step 4100 | Loss: 2.1206 | LR: 2.26e-04 | Tokens/s: 31321\n",
            "Model saved to best_model.npz\n",
            "Config saved to best_model_config.json\n",
            "Checkpoint saved: best_model.npz\n",
            "Epoch 2/5 | Step 4110 | Loss: 2.9994 | LR: 2.26e-04 | Tokens/s: 31327\n",
            "Epoch 2/5 | Step 4120 | Loss: 3.1109 | LR: 2.26e-04 | Tokens/s: 31334\n",
            "Epoch 2/5 | Step 4130 | Loss: 2.7449 | LR: 2.25e-04 | Tokens/s: 31340\n",
            "Epoch 2/5 | Step 4140 | Loss: 2.8844 | LR: 2.25e-04 | Tokens/s: 31348\n",
            "Epoch 2/5 | Step 4150 | Loss: 2.6698 | LR: 2.25e-04 | Tokens/s: 31356\n",
            "Epoch 2/5 | Step 4160 | Loss: 2.6337 | LR: 2.24e-04 | Tokens/s: 31364\n",
            "Epoch 2/5 | Step 4170 | Loss: 3.0466 | LR: 2.24e-04 | Tokens/s: 31371\n",
            "Epoch 2/5 | Step 4180 | Loss: 3.4159 | LR: 2.24e-04 | Tokens/s: 31373\n",
            "Epoch 2/5 | Step 4190 | Loss: 3.1702 | LR: 2.23e-04 | Tokens/s: 31379\n",
            "Epoch 2/5 | Step 4200 | Loss: 2.8782 | LR: 2.23e-04 | Tokens/s: 31386\n",
            "Epoch 2/5 | Step 4210 | Loss: 2.6258 | LR: 2.23e-04 | Tokens/s: 31393\n",
            "Epoch 2/5 | Step 4220 | Loss: 2.9010 | LR: 2.22e-04 | Tokens/s: 31400\n",
            "Epoch 2/5 | Step 4230 | Loss: 2.3414 | LR: 2.22e-04 | Tokens/s: 31390\n",
            "Epoch 2/5 | Step 4240 | Loss: 3.0116 | LR: 2.22e-04 | Tokens/s: 31373\n",
            "Epoch 2/5 | Step 4250 | Loss: 2.3135 | LR: 2.21e-04 | Tokens/s: 31352\n",
            "Epoch 2/5 | Step 4260 | Loss: 2.8408 | LR: 2.21e-04 | Tokens/s: 31332\n",
            "Epoch 2/5 | Step 4270 | Loss: 2.3350 | LR: 2.21e-04 | Tokens/s: 31329\n",
            "Epoch 2/5 | Step 4280 | Loss: 3.1575 | LR: 2.20e-04 | Tokens/s: 31330\n",
            "Epoch 2/5 | Step 4290 | Loss: 2.3720 | LR: 2.20e-04 | Tokens/s: 31335\n",
            "Epoch 2/5 | Step 4300 | Loss: 2.7034 | LR: 2.20e-04 | Tokens/s: 31336\n",
            "Epoch 2/5 | Step 4310 | Loss: 3.5434 | LR: 2.19e-04 | Tokens/s: 31341\n",
            "Epoch 2/5 | Step 4320 | Loss: 2.5298 | LR: 2.19e-04 | Tokens/s: 31347\n",
            "Epoch 2/5 | Step 4330 | Loss: 2.6828 | LR: 2.19e-04 | Tokens/s: 31353\n",
            "Epoch 2/5 | Step 4340 | Loss: 2.6827 | LR: 2.18e-04 | Tokens/s: 31362\n",
            "Epoch 2/5 | Step 4350 | Loss: 2.5976 | LR: 2.18e-04 | Tokens/s: 31368\n",
            "Epoch 2/5 | Step 4360 | Loss: 3.1890 | LR: 2.18e-04 | Tokens/s: 31375\n",
            "Epoch 2/5 | Step 4370 | Loss: 3.7163 | LR: 2.17e-04 | Tokens/s: 31381\n",
            "Epoch 2/5 | Step 4380 | Loss: 2.2015 | LR: 2.17e-04 | Tokens/s: 31388\n",
            "Epoch 2/5 | Step 4390 | Loss: 3.5105 | LR: 2.16e-04 | Tokens/s: 31393\n",
            "Epoch 2/5 | Step 4400 | Loss: 2.3814 | LR: 2.16e-04 | Tokens/s: 31400\n",
            "Epoch 2/5 | Step 4410 | Loss: 2.2050 | LR: 2.16e-04 | Tokens/s: 31408\n",
            "Epoch 2/5 | Step 4420 | Loss: 3.3164 | LR: 2.15e-04 | Tokens/s: 31410\n",
            "Epoch 2/5 | Step 4430 | Loss: 2.5932 | LR: 2.15e-04 | Tokens/s: 31409\n",
            "Epoch 2/5 | Step 4440 | Loss: 3.0589 | LR: 2.15e-04 | Tokens/s: 31392\n",
            "Epoch 2/5 | Step 4450 | Loss: 2.1913 | LR: 2.14e-04 | Tokens/s: 31372\n",
            "Epoch 2/5 | Step 4460 | Loss: 2.7771 | LR: 2.14e-04 | Tokens/s: 31354\n",
            "Epoch 2/5 | Step 4470 | Loss: 2.4858 | LR: 2.14e-04 | Tokens/s: 31340\n",
            "Epoch 2/5 | Step 4480 | Loss: 2.2302 | LR: 2.13e-04 | Tokens/s: 31346\n",
            "Epoch 2/5 | Step 4490 | Loss: 2.9648 | LR: 2.13e-04 | Tokens/s: 31350\n",
            "Epoch 2/5 | Step 4500 | Loss: 3.0748 | LR: 2.13e-04 | Tokens/s: 31357\n",
            "Epoch 2/5 | Step 4510 | Loss: 2.8394 | LR: 2.12e-04 | Tokens/s: 31363\n",
            "Epoch 2/5 | Step 4520 | Loss: 2.9731 | LR: 2.12e-04 | Tokens/s: 31370\n",
            "Epoch 2/5 | Step 4530 | Loss: 3.1951 | LR: 2.12e-04 | Tokens/s: 31376\n",
            "Epoch 2/5 | Step 4540 | Loss: 2.8600 | LR: 2.11e-04 | Tokens/s: 31375\n",
            "Epoch 2/5 | Step 4550 | Loss: 2.8466 | LR: 2.11e-04 | Tokens/s: 31383\n",
            "Epoch 2/5 | Step 4560 | Loss: 2.7199 | LR: 2.10e-04 | Tokens/s: 31389\n",
            "Epoch 2/5 | Step 4570 | Loss: 2.7229 | LR: 2.10e-04 | Tokens/s: 31396\n",
            "Epoch 2/5 | Step 4580 | Loss: 2.5255 | LR: 2.10e-04 | Tokens/s: 31403\n",
            "Epoch 2/5 | Step 4590 | Loss: 3.2062 | LR: 2.09e-04 | Tokens/s: 31410\n",
            "Epoch 2/5 | Step 4600 | Loss: 3.2327 | LR: 2.09e-04 | Tokens/s: 31417\n",
            "Epoch 2/5 | Step 4610 | Loss: 3.1147 | LR: 2.09e-04 | Tokens/s: 31425\n",
            "Epoch 2/5 | Step 4620 | Loss: 3.1819 | LR: 2.08e-04 | Tokens/s: 31432\n",
            "Epoch 2/5 | Step 4630 | Loss: 3.6438 | LR: 2.08e-04 | Tokens/s: 31436\n",
            "Epoch 2/5 | Step 4640 | Loss: 2.1564 | LR: 2.08e-04 | Tokens/s: 31427\n",
            "Epoch 2/5 | Step 4650 | Loss: 2.9713 | LR: 2.07e-04 | Tokens/s: 31412\n",
            "Epoch 2/5 | Step 4660 | Loss: 3.2042 | LR: 2.07e-04 | Tokens/s: 31396\n",
            "Epoch 2/5 | Step 4670 | Loss: 3.2867 | LR: 2.07e-04 | Tokens/s: 31365\n",
            "Epoch 2/5 | Step 4680 | Loss: 2.9805 | LR: 2.06e-04 | Tokens/s: 31372\n",
            "Epoch 2/5 | Step 4690 | Loss: 3.1069 | LR: 2.06e-04 | Tokens/s: 31379\n",
            "Epoch 2/5 | Step 4700 | Loss: 4.7534 | LR: 2.05e-04 | Tokens/s: 31387\n",
            "Epoch 2/5 | Step 4710 | Loss: 2.9227 | LR: 2.05e-04 | Tokens/s: 31393\n",
            "Epoch 2/5 | Step 4720 | Loss: 2.7270 | LR: 2.05e-04 | Tokens/s: 31399\n",
            "Epoch 2/5 | Step 4730 | Loss: 3.1665 | LR: 2.04e-04 | Tokens/s: 31407\n",
            "Epoch 2/5 | Step 4740 | Loss: 5.0059 | LR: 2.04e-04 | Tokens/s: 31413\n",
            "Epoch 2/5 | Step 4750 | Loss: 4.4438 | LR: 2.04e-04 | Tokens/s: 31421\n",
            "Epoch 2/5 | Step 4760 | Loss: 4.5023 | LR: 2.03e-04 | Tokens/s: 31424\n",
            "Epoch 2/5 | Step 4770 | Loss: 4.3333 | LR: 2.03e-04 | Tokens/s: 31432\n",
            "Epoch 2/5 | Step 4780 | Loss: 3.9396 | LR: 2.03e-04 | Tokens/s: 31438\n",
            "Epoch 2/5 | Step 4790 | Loss: 4.0511 | LR: 2.02e-04 | Tokens/s: 31437\n",
            "Epoch 2/5 | Step 4800 | Loss: 3.7892 | LR: 2.02e-04 | Tokens/s: 31443\n",
            "Epoch 2/5 | Step 4810 | Loss: 3.8653 | LR: 2.01e-04 | Tokens/s: 31450\n",
            "Epoch 2/5 | Step 4820 | Loss: 3.7786 | LR: 2.01e-04 | Tokens/s: 31458\n",
            "Epoch 2/5 | Step 4830 | Loss: 4.0389 | LR: 2.01e-04 | Tokens/s: 31464\n",
            "Epoch 2/5 | Step 4840 | Loss: 3.7756 | LR: 2.00e-04 | Tokens/s: 31471\n",
            "Epoch 2/5 | Step 4850 | Loss: 3.5087 | LR: 2.00e-04 | Tokens/s: 31454\n",
            "Epoch 2/5 | Step 4860 | Loss: 3.4056 | LR: 2.00e-04 | Tokens/s: 31437\n",
            "Epoch 2/5 | Step 4870 | Loss: 3.8145 | LR: 1.99e-04 | Tokens/s: 31424\n",
            "Epoch 2/5 | Step 4880 | Loss: 3.5822 | LR: 1.99e-04 | Tokens/s: 31409\n",
            "\n",
            "========================================\n",
            "Epoch 2 complete | Average Loss: 3.0000\n",
            "========================================\n",
            "\n",
            "Epoch 3/5 | Step 4890 | Loss: 4.4686 | LR: 1.99e-04 | Tokens/s: 31414\n",
            "Epoch 3/5 | Step 4900 | Loss: 4.2622 | LR: 1.98e-04 | Tokens/s: 31420\n",
            "Epoch 3/5 | Step 4910 | Loss: 4.3606 | LR: 1.98e-04 | Tokens/s: 31423\n",
            "Epoch 3/5 | Step 4920 | Loss: 4.1886 | LR: 1.97e-04 | Tokens/s: 31429\n",
            "Epoch 3/5 | Step 4930 | Loss: 3.7380 | LR: 1.97e-04 | Tokens/s: 31437\n",
            "Epoch 3/5 | Step 4940 | Loss: 3.3610 | LR: 1.97e-04 | Tokens/s: 31443\n",
            "Epoch 3/5 | Step 4950 | Loss: 3.2458 | LR: 1.96e-04 | Tokens/s: 31449\n",
            "Epoch 3/5 | Step 4960 | Loss: 3.0537 | LR: 1.96e-04 | Tokens/s: 31454\n",
            "Epoch 3/5 | Step 4970 | Loss: 3.1204 | LR: 1.96e-04 | Tokens/s: 31460\n",
            "Epoch 3/5 | Step 4980 | Loss: 3.1630 | LR: 1.95e-04 | Tokens/s: 31467\n",
            "Epoch 3/5 | Step 4990 | Loss: 3.1510 | LR: 1.95e-04 | Tokens/s: 31472\n",
            "Epoch 3/5 | Step 5000 | Loss: 2.6178 | LR: 1.94e-04 | Tokens/s: 31479\n",
            "Epoch 3/5 | Step 5010 | Loss: 3.2380 | LR: 1.94e-04 | Tokens/s: 31484\n",
            "Epoch 3/5 | Step 5020 | Loss: 3.7160 | LR: 1.94e-04 | Tokens/s: 31491\n",
            "Epoch 3/5 | Step 5030 | Loss: 3.1000 | LR: 1.93e-04 | Tokens/s: 31492\n",
            "Epoch 3/5 | Step 5040 | Loss: 3.0134 | LR: 1.93e-04 | Tokens/s: 31498\n",
            "Epoch 3/5 | Step 5050 | Loss: 3.0401 | LR: 1.93e-04 | Tokens/s: 31492\n",
            "Epoch 3/5 | Step 5060 | Loss: 2.8508 | LR: 1.92e-04 | Tokens/s: 31478\n",
            "Epoch 3/5 | Step 5070 | Loss: 3.0249 | LR: 1.92e-04 | Tokens/s: 31459\n",
            "Epoch 3/5 | Step 5080 | Loss: 3.1364 | LR: 1.92e-04 | Tokens/s: 31433\n",
            "Epoch 3/5 | Step 5090 | Loss: 3.7599 | LR: 1.91e-04 | Tokens/s: 31425\n",
            "Epoch 3/5 | Step 5100 | Loss: 2.8970 | LR: 1.91e-04 | Tokens/s: 31429\n",
            "Epoch 3/5 | Step 5110 | Loss: 3.0894 | LR: 1.90e-04 | Tokens/s: 31429\n",
            "Epoch 3/5 | Step 5120 | Loss: 3.2441 | LR: 1.90e-04 | Tokens/s: 31432\n",
            "Epoch 3/5 | Step 5130 | Loss: 3.0010 | LR: 1.90e-04 | Tokens/s: 31437\n",
            "Epoch 3/5 | Step 5140 | Loss: 3.0603 | LR: 1.89e-04 | Tokens/s: 31439\n",
            "Epoch 3/5 | Step 5150 | Loss: 2.8669 | LR: 1.89e-04 | Tokens/s: 31440\n",
            "Epoch 3/5 | Step 5160 | Loss: 3.0008 | LR: 1.89e-04 | Tokens/s: 31445\n",
            "Epoch 3/5 | Step 5170 | Loss: 2.8457 | LR: 1.88e-04 | Tokens/s: 31451\n",
            "Epoch 3/5 | Step 5180 | Loss: 2.8329 | LR: 1.88e-04 | Tokens/s: 31457\n",
            "Epoch 3/5 | Step 5190 | Loss: 2.9107 | LR: 1.87e-04 | Tokens/s: 31461\n",
            "Epoch 3/5 | Step 5200 | Loss: 2.9117 | LR: 1.87e-04 | Tokens/s: 31466\n",
            "Epoch 3/5 | Step 5210 | Loss: 2.7216 | LR: 1.87e-04 | Tokens/s: 31471\n",
            "Epoch 3/5 | Step 5220 | Loss: 2.9919 | LR: 1.86e-04 | Tokens/s: 31474\n",
            "Epoch 3/5 | Step 5230 | Loss: 2.6705 | LR: 1.86e-04 | Tokens/s: 31476\n",
            "Epoch 3/5 | Step 5240 | Loss: 2.7844 | LR: 1.85e-04 | Tokens/s: 31481\n",
            "Epoch 3/5 | Step 5250 | Loss: 2.9425 | LR: 1.85e-04 | Tokens/s: 31476\n",
            "Epoch 3/5 | Step 5260 | Loss: 2.6081 | LR: 1.85e-04 | Tokens/s: 31462\n",
            "Epoch 3/5 | Step 5270 | Loss: 2.7020 | LR: 1.84e-04 | Tokens/s: 31434\n",
            "Epoch 3/5 | Step 5280 | Loss: 2.7228 | LR: 1.84e-04 | Tokens/s: 31409\n",
            "Epoch 3/5 | Step 5290 | Loss: 2.6863 | LR: 1.84e-04 | Tokens/s: 31397\n",
            "Epoch 3/5 | Step 5300 | Loss: 2.4811 | LR: 1.83e-04 | Tokens/s: 31393\n",
            "Epoch 3/5 | Step 5310 | Loss: 2.9445 | LR: 1.83e-04 | Tokens/s: 31396\n",
            "Epoch 3/5 | Step 5320 | Loss: 2.7132 | LR: 1.82e-04 | Tokens/s: 31399\n",
            "Epoch 3/5 | Step 5330 | Loss: 2.6060 | LR: 1.82e-04 | Tokens/s: 31400\n",
            "Epoch 3/5 | Step 5340 | Loss: 2.4351 | LR: 1.82e-04 | Tokens/s: 31404\n",
            "Epoch 3/5 | Step 5350 | Loss: 3.9809 | LR: 1.81e-04 | Tokens/s: 31408\n",
            "Epoch 3/5 | Step 5360 | Loss: 3.5592 | LR: 1.81e-04 | Tokens/s: 31415\n",
            "Epoch 3/5 | Step 5370 | Loss: 2.6547 | LR: 1.81e-04 | Tokens/s: 31420\n",
            "Epoch 3/5 | Step 5380 | Loss: 3.1318 | LR: 1.80e-04 | Tokens/s: 31424\n",
            "Epoch 3/5 | Step 5390 | Loss: 3.0633 | LR: 1.80e-04 | Tokens/s: 31427\n",
            "Epoch 3/5 | Step 5400 | Loss: 3.0172 | LR: 1.79e-04 | Tokens/s: 31430\n",
            "Epoch 3/5 | Step 5410 | Loss: 2.8410 | LR: 1.79e-04 | Tokens/s: 31434\n",
            "Epoch 3/5 | Step 5420 | Loss: 2.7563 | LR: 1.79e-04 | Tokens/s: 31437\n",
            "Epoch 3/5 | Step 5430 | Loss: 3.1600 | LR: 1.78e-04 | Tokens/s: 31443\n",
            "Epoch 3/5 | Step 5440 | Loss: 2.8333 | LR: 1.78e-04 | Tokens/s: 31443\n",
            "Epoch 3/5 | Step 5450 | Loss: 5.0114 | LR: 1.78e-04 | Tokens/s: 31435\n",
            "Epoch 3/5 | Step 5460 | Loss: 3.6518 | LR: 1.77e-04 | Tokens/s: 31420\n",
            "Epoch 3/5 | Step 5470 | Loss: 2.3214 | LR: 1.77e-04 | Tokens/s: 31400\n",
            "Epoch 3/5 | Step 5480 | Loss: 2.9178 | LR: 1.76e-04 | Tokens/s: 31381\n",
            "Epoch 3/5 | Step 5490 | Loss: 2.9811 | LR: 1.76e-04 | Tokens/s: 31373\n",
            "Epoch 3/5 | Step 5500 | Loss: 3.0335 | LR: 1.76e-04 | Tokens/s: 31378\n",
            "Epoch 3/5 | Step 5510 | Loss: 2.7804 | LR: 1.75e-04 | Tokens/s: 31377\n",
            "Epoch 3/5 | Step 5520 | Loss: 2.9406 | LR: 1.75e-04 | Tokens/s: 31382\n",
            "Epoch 3/5 | Step 5530 | Loss: 3.7277 | LR: 1.74e-04 | Tokens/s: 31388\n",
            "Epoch 3/5 | Step 5540 | Loss: 2.8066 | LR: 1.74e-04 | Tokens/s: 31392\n",
            "Epoch 3/5 | Step 5550 | Loss: 3.1980 | LR: 1.74e-04 | Tokens/s: 31397\n",
            "Epoch 3/5 | Step 5560 | Loss: 3.1298 | LR: 1.73e-04 | Tokens/s: 31402\n",
            "Epoch 3/5 | Step 5570 | Loss: 2.9977 | LR: 1.73e-04 | Tokens/s: 31407\n",
            "Epoch 3/5 | Step 5580 | Loss: 2.8698 | LR: 1.73e-04 | Tokens/s: 31414\n",
            "Epoch 3/5 | Step 5590 | Loss: 2.8255 | LR: 1.72e-04 | Tokens/s: 31418\n",
            "Epoch 3/5 | Step 5600 | Loss: 2.9790 | LR: 1.72e-04 | Tokens/s: 31422\n",
            "Epoch 3/5 | Step 5610 | Loss: 2.8913 | LR: 1.71e-04 | Tokens/s: 31425\n",
            "Epoch 3/5 | Step 5620 | Loss: 3.2259 | LR: 1.71e-04 | Tokens/s: 31430\n",
            "Epoch 3/5 | Step 5630 | Loss: 3.0422 | LR: 1.71e-04 | Tokens/s: 31431\n",
            "Epoch 3/5 | Step 5640 | Loss: 2.7354 | LR: 1.70e-04 | Tokens/s: 31436\n",
            "Epoch 3/5 | Step 5650 | Loss: 3.2215 | LR: 1.70e-04 | Tokens/s: 31435\n",
            "Epoch 3/5 | Step 5660 | Loss: 2.6224 | LR: 1.69e-04 | Tokens/s: 31422\n",
            "Epoch 3/5 | Step 5670 | Loss: 2.8203 | LR: 1.69e-04 | Tokens/s: 31405\n",
            "Epoch 3/5 | Step 5680 | Loss: 2.3367 | LR: 1.69e-04 | Tokens/s: 31386\n",
            "Epoch 3/5 | Step 5690 | Loss: 2.4375 | LR: 1.68e-04 | Tokens/s: 31373\n",
            "Epoch 3/5 | Step 5700 | Loss: 2.7131 | LR: 1.68e-04 | Tokens/s: 31376\n",
            "Epoch 3/5 | Step 5710 | Loss: 2.9021 | LR: 1.68e-04 | Tokens/s: 31379\n",
            "Epoch 3/5 | Step 5720 | Loss: 2.7399 | LR: 1.67e-04 | Tokens/s: 31384\n",
            "Epoch 3/5 | Step 5730 | Loss: 2.9805 | LR: 1.67e-04 | Tokens/s: 31387\n",
            "Epoch 3/5 | Step 5740 | Loss: 2.8309 | LR: 1.66e-04 | Tokens/s: 31392\n",
            "Epoch 3/5 | Step 5750 | Loss: 2.8374 | LR: 1.66e-04 | Tokens/s: 31390\n",
            "Epoch 3/5 | Step 5760 | Loss: 2.7331 | LR: 1.66e-04 | Tokens/s: 31391\n",
            "Epoch 3/5 | Step 5770 | Loss: 2.6987 | LR: 1.65e-04 | Tokens/s: 31393\n",
            "Epoch 3/5 | Step 5780 | Loss: 2.5477 | LR: 1.65e-04 | Tokens/s: 31395\n",
            "Epoch 3/5 | Step 5790 | Loss: 2.6298 | LR: 1.64e-04 | Tokens/s: 31399\n",
            "Epoch 3/5 | Step 5800 | Loss: 2.4331 | LR: 1.64e-04 | Tokens/s: 31402\n",
            "Epoch 3/5 | Step 5810 | Loss: 2.7777 | LR: 1.64e-04 | Tokens/s: 31404\n",
            "Epoch 3/5 | Step 5820 | Loss: 2.8816 | LR: 1.63e-04 | Tokens/s: 31408\n",
            "Epoch 3/5 | Step 5830 | Loss: 2.4904 | LR: 1.63e-04 | Tokens/s: 31409\n",
            "Epoch 3/5 | Step 5840 | Loss: 3.0868 | LR: 1.63e-04 | Tokens/s: 31415\n",
            "Epoch 3/5 | Step 5850 | Loss: 3.6617 | LR: 1.62e-04 | Tokens/s: 31410\n",
            "Epoch 3/5 | Step 5860 | Loss: 3.2876 | LR: 1.62e-04 | Tokens/s: 31397\n",
            "Epoch 3/5 | Step 5870 | Loss: 3.2954 | LR: 1.61e-04 | Tokens/s: 31378\n",
            "Epoch 3/5 | Step 5880 | Loss: 2.6815 | LR: 1.61e-04 | Tokens/s: 31358\n",
            "Epoch 3/5 | Step 5890 | Loss: 3.3237 | LR: 1.61e-04 | Tokens/s: 31352\n",
            "Epoch 3/5 | Step 5900 | Loss: 3.1223 | LR: 1.60e-04 | Tokens/s: 31356\n",
            "Epoch 3/5 | Step 5910 | Loss: 3.2543 | LR: 1.60e-04 | Tokens/s: 31360\n",
            "Epoch 3/5 | Step 5920 | Loss: 2.5747 | LR: 1.59e-04 | Tokens/s: 31365\n",
            "Epoch 3/5 | Step 5930 | Loss: 3.0109 | LR: 1.59e-04 | Tokens/s: 31370\n",
            "Epoch 3/5 | Step 5940 | Loss: 3.1467 | LR: 1.59e-04 | Tokens/s: 31374\n",
            "Epoch 3/5 | Step 5950 | Loss: 3.2024 | LR: 1.58e-04 | Tokens/s: 31378\n",
            "Epoch 3/5 | Step 5960 | Loss: 2.9073 | LR: 1.58e-04 | Tokens/s: 31383\n",
            "Epoch 3/5 | Step 5970 | Loss: 2.7410 | LR: 1.57e-04 | Tokens/s: 31385\n",
            "Epoch 3/5 | Step 5980 | Loss: 2.6713 | LR: 1.57e-04 | Tokens/s: 31390\n",
            "Epoch 3/5 | Step 5990 | Loss: 2.3324 | LR: 1.57e-04 | Tokens/s: 31393\n",
            "Epoch 3/5 | Step 6000 | Loss: 2.6328 | LR: 1.56e-04 | Tokens/s: 31393\n",
            "Epoch 3/5 | Step 6010 | Loss: 2.5809 | LR: 1.56e-04 | Tokens/s: 31398\n",
            "Epoch 3/5 | Step 6020 | Loss: 2.8494 | LR: 1.56e-04 | Tokens/s: 31402\n",
            "Epoch 3/5 | Step 6030 | Loss: 2.7958 | LR: 1.55e-04 | Tokens/s: 31406\n",
            "Epoch 3/5 | Step 6040 | Loss: 2.5093 | LR: 1.55e-04 | Tokens/s: 31408\n",
            "Epoch 3/5 | Step 6050 | Loss: 2.5652 | LR: 1.54e-04 | Tokens/s: 31406\n",
            "Epoch 3/5 | Step 6060 | Loss: 2.5400 | LR: 1.54e-04 | Tokens/s: 31393\n",
            "Epoch 3/5 | Step 6070 | Loss: 2.5748 | LR: 1.54e-04 | Tokens/s: 31377\n",
            "Epoch 3/5 | Step 6080 | Loss: 2.6630 | LR: 1.53e-04 | Tokens/s: 31361\n",
            "Epoch 3/5 | Step 6090 | Loss: 2.6194 | LR: 1.53e-04 | Tokens/s: 31351\n",
            "Epoch 3/5 | Step 6100 | Loss: 3.0300 | LR: 1.52e-04 | Tokens/s: 31356\n",
            "Epoch 3/5 | Step 6110 | Loss: 2.6561 | LR: 1.52e-04 | Tokens/s: 31360\n",
            "Epoch 3/5 | Step 6120 | Loss: 2.4711 | LR: 1.52e-04 | Tokens/s: 31362\n",
            "Epoch 3/5 | Step 6130 | Loss: 2.2955 | LR: 1.51e-04 | Tokens/s: 31364\n",
            "Epoch 3/5 | Step 6140 | Loss: 2.5638 | LR: 1.51e-04 | Tokens/s: 31369\n",
            "Epoch 3/5 | Step 6150 | Loss: 2.6629 | LR: 1.50e-04 | Tokens/s: 31374\n",
            "Epoch 3/5 | Step 6160 | Loss: 2.4932 | LR: 1.50e-04 | Tokens/s: 31379\n",
            "Epoch 3/5 | Step 6170 | Loss: 2.5754 | LR: 1.50e-04 | Tokens/s: 31384\n",
            "Epoch 3/5 | Step 6180 | Loss: 2.5467 | LR: 1.49e-04 | Tokens/s: 31388\n",
            "Epoch 3/5 | Step 6190 | Loss: 2.5745 | LR: 1.49e-04 | Tokens/s: 31393\n",
            "Epoch 3/5 | Step 6200 | Loss: 2.6332 | LR: 1.49e-04 | Tokens/s: 31397\n",
            "Epoch 3/5 | Step 6210 | Loss: 2.4631 | LR: 1.48e-04 | Tokens/s: 31401\n",
            "Epoch 3/5 | Step 6220 | Loss: 2.3404 | LR: 1.48e-04 | Tokens/s: 31406\n",
            "Epoch 3/5 | Step 6230 | Loss: 2.4365 | LR: 1.47e-04 | Tokens/s: 31410\n",
            "Epoch 3/5 | Step 6240 | Loss: 2.4179 | LR: 1.47e-04 | Tokens/s: 31411\n",
            "Epoch 3/5 | Step 6250 | Loss: 2.4679 | LR: 1.47e-04 | Tokens/s: 31416\n",
            "Epoch 3/5 | Step 6260 | Loss: 2.3434 | LR: 1.46e-04 | Tokens/s: 31400\n",
            "Epoch 3/5 | Step 6270 | Loss: 2.3757 | LR: 1.46e-04 | Tokens/s: 31385\n",
            "Epoch 3/5 | Step 6280 | Loss: 2.5593 | LR: 1.45e-04 | Tokens/s: 31375\n",
            "Epoch 3/5 | Step 6290 | Loss: 2.3751 | LR: 1.45e-04 | Tokens/s: 31358\n",
            "Epoch 3/5 | Step 6300 | Loss: 3.2975 | LR: 1.45e-04 | Tokens/s: 31362\n",
            "Epoch 3/5 | Step 6310 | Loss: 2.8523 | LR: 1.44e-04 | Tokens/s: 31367\n",
            "Epoch 3/5 | Step 6320 | Loss: 3.0710 | LR: 1.44e-04 | Tokens/s: 31372\n",
            "Epoch 3/5 | Step 6330 | Loss: 3.0670 | LR: 1.44e-04 | Tokens/s: 31377\n",
            "Epoch 3/5 | Step 6340 | Loss: 2.8923 | LR: 1.43e-04 | Tokens/s: 31380\n",
            "Epoch 3/5 | Step 6350 | Loss: 2.9402 | LR: 1.43e-04 | Tokens/s: 31383\n",
            "Epoch 3/5 | Step 6360 | Loss: 3.0062 | LR: 1.42e-04 | Tokens/s: 31386\n",
            "Epoch 3/5 | Step 6370 | Loss: 2.6242 | LR: 1.42e-04 | Tokens/s: 31390\n",
            "Epoch 3/5 | Step 6380 | Loss: 2.6649 | LR: 1.42e-04 | Tokens/s: 31394\n",
            "Epoch 3/5 | Step 6390 | Loss: 2.6111 | LR: 1.41e-04 | Tokens/s: 31398\n",
            "Epoch 3/5 | Step 6400 | Loss: 2.7726 | LR: 1.41e-04 | Tokens/s: 31402\n",
            "Epoch 3/5 | Step 6410 | Loss: 2.7973 | LR: 1.40e-04 | Tokens/s: 31408\n",
            "Epoch 3/5 | Step 6420 | Loss: 2.4027 | LR: 1.40e-04 | Tokens/s: 31412\n",
            "Epoch 3/5 | Step 6430 | Loss: 2.7023 | LR: 1.40e-04 | Tokens/s: 31417\n",
            "Epoch 3/5 | Step 6440 | Loss: 3.0317 | LR: 1.39e-04 | Tokens/s: 31422\n",
            "Epoch 3/5 | Step 6450 | Loss: 2.7501 | LR: 1.39e-04 | Tokens/s: 31426\n",
            "Epoch 3/5 | Step 6460 | Loss: 2.9997 | LR: 1.38e-04 | Tokens/s: 31422\n",
            "Epoch 3/5 | Step 6470 | Loss: 2.5630 | LR: 1.38e-04 | Tokens/s: 31412\n",
            "Epoch 3/5 | Step 6480 | Loss: 2.7447 | LR: 1.38e-04 | Tokens/s: 31394\n",
            "Epoch 3/5 | Step 6490 | Loss: 2.5144 | LR: 1.37e-04 | Tokens/s: 31381\n",
            "Epoch 3/5 | Step 6500 | Loss: 2.6437 | LR: 1.37e-04 | Tokens/s: 31377\n",
            "Epoch 3/5 | Step 6510 | Loss: 2.4111 | LR: 1.37e-04 | Tokens/s: 31381\n",
            "Epoch 3/5 | Step 6520 | Loss: 2.4427 | LR: 1.36e-04 | Tokens/s: 31387\n",
            "Epoch 3/5 | Step 6530 | Loss: 2.3563 | LR: 1.36e-04 | Tokens/s: 31391\n",
            "Epoch 3/5 | Step 6540 | Loss: 2.1548 | LR: 1.35e-04 | Tokens/s: 31396\n",
            "Epoch 3/5 | Step 6550 | Loss: 2.9013 | LR: 1.35e-04 | Tokens/s: 31399\n",
            "Epoch 3/5 | Step 6560 | Loss: 3.0226 | LR: 1.35e-04 | Tokens/s: 31404\n",
            "Epoch 3/5 | Step 6570 | Loss: 2.6271 | LR: 1.34e-04 | Tokens/s: 31408\n",
            "Epoch 3/5 | Step 6580 | Loss: 2.8838 | LR: 1.34e-04 | Tokens/s: 31412\n",
            "Epoch 3/5 | Step 6590 | Loss: 2.6618 | LR: 1.33e-04 | Tokens/s: 31417\n",
            "Epoch 3/5 | Step 6600 | Loss: 2.5193 | LR: 1.33e-04 | Tokens/s: 31419\n",
            "Epoch 3/5 | Step 6610 | Loss: 3.0176 | LR: 1.33e-04 | Tokens/s: 31422\n",
            "Epoch 3/5 | Step 6620 | Loss: 3.1431 | LR: 1.32e-04 | Tokens/s: 31426\n",
            "Epoch 3/5 | Step 6630 | Loss: 3.0711 | LR: 1.32e-04 | Tokens/s: 31430\n",
            "Epoch 3/5 | Step 6640 | Loss: 2.8183 | LR: 1.32e-04 | Tokens/s: 31435\n",
            "Epoch 3/5 | Step 6650 | Loss: 2.6048 | LR: 1.31e-04 | Tokens/s: 31439\n",
            "Epoch 3/5 | Step 6660 | Loss: 2.9599 | LR: 1.31e-04 | Tokens/s: 31444\n",
            "Epoch 3/5 | Step 6670 | Loss: 2.4214 | LR: 1.30e-04 | Tokens/s: 31432\n",
            "Epoch 3/5 | Step 6680 | Loss: 2.9718 | LR: 1.30e-04 | Tokens/s: 31419\n",
            "Epoch 3/5 | Step 6690 | Loss: 2.4151 | LR: 1.30e-04 | Tokens/s: 31409\n",
            "Epoch 3/5 | Step 6700 | Loss: 2.9700 | LR: 1.29e-04 | Tokens/s: 31393\n",
            "Epoch 3/5 | Step 6710 | Loss: 2.3817 | LR: 1.29e-04 | Tokens/s: 31391\n",
            "Epoch 3/5 | Step 6720 | Loss: 3.0705 | LR: 1.28e-04 | Tokens/s: 31387\n",
            "Epoch 3/5 | Step 6730 | Loss: 2.5715 | LR: 1.28e-04 | Tokens/s: 31391\n",
            "Epoch 3/5 | Step 6740 | Loss: 2.7404 | LR: 1.28e-04 | Tokens/s: 31393\n",
            "Epoch 3/5 | Step 6750 | Loss: 3.5865 | LR: 1.27e-04 | Tokens/s: 31395\n",
            "Epoch 3/5 | Step 6760 | Loss: 2.6129 | LR: 1.27e-04 | Tokens/s: 31399\n",
            "Epoch 3/5 | Step 6770 | Loss: 2.7379 | LR: 1.27e-04 | Tokens/s: 31401\n",
            "Epoch 3/5 | Step 6780 | Loss: 2.7387 | LR: 1.26e-04 | Tokens/s: 31406\n",
            "Epoch 3/5 | Step 6790 | Loss: 2.6109 | LR: 1.26e-04 | Tokens/s: 31410\n",
            "Epoch 3/5 | Step 6800 | Loss: 3.2989 | LR: 1.25e-04 | Tokens/s: 31415\n",
            "Epoch 3/5 | Step 6810 | Loss: 3.8657 | LR: 1.25e-04 | Tokens/s: 31419\n",
            "Epoch 3/5 | Step 6820 | Loss: 2.3757 | LR: 1.25e-04 | Tokens/s: 31423\n",
            "Epoch 3/5 | Step 6830 | Loss: 3.5620 | LR: 1.24e-04 | Tokens/s: 31426\n",
            "Epoch 3/5 | Step 6840 | Loss: 2.5652 | LR: 1.24e-04 | Tokens/s: 31427\n",
            "Epoch 3/5 | Step 6850 | Loss: 2.3123 | LR: 1.23e-04 | Tokens/s: 31431\n",
            "Epoch 3/5 | Step 6860 | Loss: 3.5851 | LR: 1.23e-04 | Tokens/s: 31432\n",
            "Epoch 3/5 | Step 6870 | Loss: 2.7652 | LR: 1.23e-04 | Tokens/s: 31418\n",
            "Epoch 3/5 | Step 6880 | Loss: 3.2298 | LR: 1.22e-04 | Tokens/s: 31403\n",
            "Epoch 3/5 | Step 6890 | Loss: 2.4554 | LR: 1.22e-04 | Tokens/s: 31391\n",
            "Epoch 3/5 | Step 6900 | Loss: 2.9682 | LR: 1.22e-04 | Tokens/s: 31379\n",
            "Epoch 3/5 | Step 6910 | Loss: 2.5692 | LR: 1.21e-04 | Tokens/s: 31382\n",
            "Epoch 3/5 | Step 6920 | Loss: 2.3542 | LR: 1.21e-04 | Tokens/s: 31384\n",
            "Epoch 3/5 | Step 6930 | Loss: 3.2525 | LR: 1.20e-04 | Tokens/s: 31385\n",
            "Epoch 3/5 | Step 6940 | Loss: 3.2979 | LR: 1.20e-04 | Tokens/s: 31390\n",
            "Epoch 3/5 | Step 6950 | Loss: 3.0997 | LR: 1.20e-04 | Tokens/s: 31394\n",
            "Epoch 3/5 | Step 6960 | Loss: 2.8936 | LR: 1.19e-04 | Tokens/s: 31395\n",
            "Epoch 3/5 | Step 6970 | Loss: 3.4823 | LR: 1.19e-04 | Tokens/s: 31400\n",
            "Epoch 3/5 | Step 6980 | Loss: 3.1853 | LR: 1.19e-04 | Tokens/s: 31404\n",
            "Epoch 3/5 | Step 6990 | Loss: 2.8846 | LR: 1.18e-04 | Tokens/s: 31409\n",
            "Epoch 3/5 | Step 7000 | Loss: 2.7906 | LR: 1.18e-04 | Tokens/s: 31413\n",
            "Epoch 3/5 | Step 7010 | Loss: 2.7660 | LR: 1.17e-04 | Tokens/s: 31417\n",
            "Epoch 3/5 | Step 7020 | Loss: 2.6809 | LR: 1.17e-04 | Tokens/s: 31421\n",
            "Epoch 3/5 | Step 7030 | Loss: 3.0758 | LR: 1.17e-04 | Tokens/s: 31424\n",
            "Epoch 3/5 | Step 7040 | Loss: 3.1294 | LR: 1.16e-04 | Tokens/s: 31429\n",
            "Epoch 3/5 | Step 7050 | Loss: 3.0634 | LR: 1.16e-04 | Tokens/s: 31432\n",
            "Epoch 3/5 | Step 7060 | Loss: 3.1116 | LR: 1.16e-04 | Tokens/s: 31437\n",
            "Epoch 3/5 | Step 7070 | Loss: 3.6015 | LR: 1.15e-04 | Tokens/s: 31425\n",
            "Epoch 3/5 | Step 7080 | Loss: 1.9980 | LR: 1.15e-04 | Tokens/s: 31412\n",
            "Epoch 3/5 | Step 7090 | Loss: 2.9220 | LR: 1.14e-04 | Tokens/s: 31397\n",
            "Epoch 3/5 | Step 7100 | Loss: 3.1580 | LR: 1.14e-04 | Tokens/s: 31383\n",
            "Epoch 3/5 | Step 7110 | Loss: 3.2587 | LR: 1.14e-04 | Tokens/s: 31387\n",
            "Epoch 3/5 | Step 7120 | Loss: 2.9352 | LR: 1.13e-04 | Tokens/s: 31390\n",
            "Epoch 3/5 | Step 7130 | Loss: 3.1283 | LR: 1.13e-04 | Tokens/s: 31395\n",
            "Epoch 3/5 | Step 7140 | Loss: 4.7264 | LR: 1.12e-04 | Tokens/s: 31399\n",
            "Epoch 3/5 | Step 7150 | Loss: 2.9886 | LR: 1.12e-04 | Tokens/s: 31403\n",
            "Epoch 3/5 | Step 7160 | Loss: 2.8144 | LR: 1.12e-04 | Tokens/s: 31407\n",
            "Epoch 3/5 | Step 7170 | Loss: 3.1381 | LR: 1.11e-04 | Tokens/s: 31411\n",
            "Epoch 3/5 | Step 7180 | Loss: 4.9787 | LR: 1.11e-04 | Tokens/s: 31414\n",
            "Epoch 3/5 | Step 7190 | Loss: 4.3816 | LR: 1.11e-04 | Tokens/s: 31418\n",
            "Epoch 3/5 | Step 7200 | Loss: 4.4816 | LR: 1.10e-04 | Tokens/s: 31423\n",
            "Epoch 3/5 | Step 7210 | Loss: 4.4216 | LR: 1.10e-04 | Tokens/s: 31423\n",
            "Epoch 3/5 | Step 7220 | Loss: 4.0219 | LR: 1.09e-04 | Tokens/s: 31427\n",
            "Epoch 3/5 | Step 7230 | Loss: 4.1116 | LR: 1.09e-04 | Tokens/s: 31431\n",
            "Epoch 3/5 | Step 7240 | Loss: 3.8732 | LR: 1.09e-04 | Tokens/s: 31435\n",
            "Epoch 3/5 | Step 7250 | Loss: 3.9879 | LR: 1.08e-04 | Tokens/s: 31439\n",
            "Epoch 3/5 | Step 7260 | Loss: 3.9063 | LR: 1.08e-04 | Tokens/s: 31443\n",
            "Epoch 3/5 | Step 7270 | Loss: 4.2359 | LR: 1.08e-04 | Tokens/s: 31442\n",
            "Epoch 3/5 | Step 7280 | Loss: 3.9647 | LR: 1.07e-04 | Tokens/s: 31431\n",
            "Epoch 3/5 | Step 7290 | Loss: 3.7249 | LR: 1.07e-04 | Tokens/s: 31418\n",
            "Epoch 3/5 | Step 7300 | Loss: 3.5949 | LR: 1.07e-04 | Tokens/s: 31406\n",
            "Epoch 3/5 | Step 7310 | Loss: 4.0171 | LR: 1.06e-04 | Tokens/s: 31401\n",
            "Epoch 3/5 | Step 7320 | Loss: 3.7706 | LR: 1.06e-04 | Tokens/s: 31404\n",
            "\n",
            "========================================\n",
            "Epoch 3 complete | Average Loss: 3.0056\n",
            "========================================\n",
            "\n",
            "Epoch 4/5 | Step 7330 | Loss: 4.5091 | LR: 1.05e-04 | Tokens/s: 31405\n",
            "Epoch 4/5 | Step 7340 | Loss: 4.3934 | LR: 1.05e-04 | Tokens/s: 31409\n",
            "Epoch 4/5 | Step 7350 | Loss: 4.4562 | LR: 1.05e-04 | Tokens/s: 31413\n",
            "Epoch 4/5 | Step 7360 | Loss: 4.3907 | LR: 1.04e-04 | Tokens/s: 31417\n",
            "Epoch 4/5 | Step 7370 | Loss: 3.6123 | LR: 1.04e-04 | Tokens/s: 31420\n",
            "Epoch 4/5 | Step 7380 | Loss: 3.2284 | LR: 1.04e-04 | Tokens/s: 31425\n",
            "Epoch 4/5 | Step 7390 | Loss: 3.1248 | LR: 1.03e-04 | Tokens/s: 31428\n",
            "Epoch 4/5 | Step 7400 | Loss: 2.9841 | LR: 1.03e-04 | Tokens/s: 31431\n",
            "Epoch 4/5 | Step 7410 | Loss: 3.0443 | LR: 1.02e-04 | Tokens/s: 31435\n",
            "Epoch 4/5 | Step 7420 | Loss: 3.1411 | LR: 1.02e-04 | Tokens/s: 31438\n",
            "Epoch 4/5 | Step 7430 | Loss: 3.1587 | LR: 1.02e-04 | Tokens/s: 31443\n",
            "Epoch 4/5 | Step 7440 | Loss: 2.5304 | LR: 1.01e-04 | Tokens/s: 31446\n",
            "Epoch 4/5 | Step 7450 | Loss: 3.2481 | LR: 1.01e-04 | Tokens/s: 31447\n",
            "Epoch 4/5 | Step 7460 | Loss: 3.7008 | LR: 1.01e-04 | Tokens/s: 31451\n",
            "Epoch 4/5 | Step 7470 | Loss: 3.0959 | LR: 1.00e-04 | Tokens/s: 31453\n",
            "Epoch 4/5 | Step 7480 | Loss: 2.9355 | LR: 9.99e-05 | Tokens/s: 31442\n",
            "Epoch 4/5 | Step 7490 | Loss: 3.0572 | LR: 9.95e-05 | Tokens/s: 31430\n",
            "Epoch 4/5 | Step 7500 | Loss: 2.8565 | LR: 9.92e-05 | Tokens/s: 31420\n",
            "Epoch 4/5 | Step 7510 | Loss: 3.0392 | LR: 9.88e-05 | Tokens/s: 31408\n",
            "Epoch 4/5 | Step 7520 | Loss: 3.0775 | LR: 9.84e-05 | Tokens/s: 31412\n",
            "Epoch 4/5 | Step 7530 | Loss: 3.5756 | LR: 9.81e-05 | Tokens/s: 31413\n",
            "Epoch 4/5 | Step 7540 | Loss: 2.8090 | LR: 9.77e-05 | Tokens/s: 31416\n",
            "Epoch 4/5 | Step 7550 | Loss: 2.9987 | LR: 9.73e-05 | Tokens/s: 31420\n",
            "Epoch 4/5 | Step 7560 | Loss: 3.1437 | LR: 9.70e-05 | Tokens/s: 31423\n",
            "Epoch 4/5 | Step 7570 | Loss: 2.9229 | LR: 9.66e-05 | Tokens/s: 31424\n",
            "Epoch 4/5 | Step 7580 | Loss: 2.9893 | LR: 9.63e-05 | Tokens/s: 31428\n",
            "Epoch 4/5 | Step 7590 | Loss: 2.8306 | LR: 9.59e-05 | Tokens/s: 31432\n",
            "Epoch 4/5 | Step 7600 | Loss: 2.9411 | LR: 9.55e-05 | Tokens/s: 31434\n",
            "Epoch 4/5 | Step 7610 | Loss: 2.7841 | LR: 9.52e-05 | Tokens/s: 31438\n",
            "Epoch 4/5 | Step 7620 | Loss: 2.7943 | LR: 9.48e-05 | Tokens/s: 31442\n",
            "Epoch 4/5 | Step 7630 | Loss: 2.8979 | LR: 9.45e-05 | Tokens/s: 31446\n",
            "Epoch 4/5 | Step 7640 | Loss: 2.9141 | LR: 9.41e-05 | Tokens/s: 31450\n",
            "Epoch 4/5 | Step 7650 | Loss: 2.7309 | LR: 9.37e-05 | Tokens/s: 31454\n",
            "Epoch 4/5 | Step 7660 | Loss: 3.0080 | LR: 9.34e-05 | Tokens/s: 31458\n",
            "Epoch 4/5 | Step 7670 | Loss: 2.6940 | LR: 9.30e-05 | Tokens/s: 31462\n",
            "Epoch 4/5 | Step 7680 | Loss: 2.8026 | LR: 9.27e-05 | Tokens/s: 31455\n",
            "Epoch 4/5 | Step 7690 | Loss: 2.9648 | LR: 9.23e-05 | Tokens/s: 31440\n",
            "Epoch 4/5 | Step 7700 | Loss: 2.6164 | LR: 9.19e-05 | Tokens/s: 31430\n",
            "Epoch 4/5 | Step 7710 | Loss: 2.6948 | LR: 9.16e-05 | Tokens/s: 31415\n",
            "Epoch 4/5 | Step 7720 | Loss: 2.7655 | LR: 9.12e-05 | Tokens/s: 31417\n",
            "Epoch 4/5 | Step 7730 | Loss: 2.7553 | LR: 9.09e-05 | Tokens/s: 31421\n",
            "Epoch 4/5 | Step 7740 | Loss: 2.4630 | LR: 9.05e-05 | Tokens/s: 31425\n",
            "Epoch 4/5 | Step 7750 | Loss: 2.9345 | LR: 9.02e-05 | Tokens/s: 31429\n",
            "Epoch 4/5 | Step 7760 | Loss: 2.6906 | LR: 8.98e-05 | Tokens/s: 31432\n",
            "Epoch 4/5 | Step 7770 | Loss: 2.5397 | LR: 8.95e-05 | Tokens/s: 31436\n",
            "Epoch 4/5 | Step 7780 | Loss: 2.4424 | LR: 8.91e-05 | Tokens/s: 31440\n",
            "Epoch 4/5 | Step 7790 | Loss: 3.8933 | LR: 8.87e-05 | Tokens/s: 31444\n",
            "Epoch 4/5 | Step 7800 | Loss: 3.5338 | LR: 8.84e-05 | Tokens/s: 31448\n",
            "Epoch 4/5 | Step 7810 | Loss: 2.5847 | LR: 8.80e-05 | Tokens/s: 31448\n",
            "Epoch 4/5 | Step 7820 | Loss: 3.0769 | LR: 8.77e-05 | Tokens/s: 31453\n",
            "Epoch 4/5 | Step 7830 | Loss: 3.0103 | LR: 8.73e-05 | Tokens/s: 31455\n",
            "Epoch 4/5 | Step 7840 | Loss: 2.9707 | LR: 8.70e-05 | Tokens/s: 31459\n",
            "Epoch 4/5 | Step 7850 | Loss: 2.8011 | LR: 8.66e-05 | Tokens/s: 31463\n",
            "Epoch 4/5 | Step 7860 | Loss: 2.7568 | LR: 8.63e-05 | Tokens/s: 31466\n",
            "Epoch 4/5 | Step 7870 | Loss: 3.1328 | LR: 8.59e-05 | Tokens/s: 31469\n",
            "Epoch 4/5 | Step 7880 | Loss: 2.8079 | LR: 8.56e-05 | Tokens/s: 31469\n",
            "Epoch 4/5 | Step 7890 | Loss: 5.1635 | LR: 8.52e-05 | Tokens/s: 31460\n",
            "Epoch 4/5 | Step 7900 | Loss: 3.4150 | LR: 8.49e-05 | Tokens/s: 31449\n",
            "Epoch 4/5 | Step 7910 | Loss: 2.1407 | LR: 8.45e-05 | Tokens/s: 31440\n",
            "Epoch 4/5 | Step 7920 | Loss: 2.8772 | LR: 8.42e-05 | Tokens/s: 31432\n",
            "Epoch 4/5 | Step 7930 | Loss: 2.9662 | LR: 8.38e-05 | Tokens/s: 31433\n",
            "Epoch 4/5 | Step 7940 | Loss: 2.9893 | LR: 8.35e-05 | Tokens/s: 31437\n",
            "Epoch 4/5 | Step 7950 | Loss: 2.7875 | LR: 8.31e-05 | Tokens/s: 31440\n",
            "Epoch 4/5 | Step 7960 | Loss: 2.9500 | LR: 8.28e-05 | Tokens/s: 31445\n",
            "Epoch 4/5 | Step 7970 | Loss: 3.7253 | LR: 8.25e-05 | Tokens/s: 31449\n",
            "Epoch 4/5 | Step 7980 | Loss: 2.8581 | LR: 8.21e-05 | Tokens/s: 31453\n",
            "Epoch 4/5 | Step 7990 | Loss: 3.1565 | LR: 8.18e-05 | Tokens/s: 31456\n",
            "Epoch 4/5 | Step 8000 | Loss: 3.1011 | LR: 8.14e-05 | Tokens/s: 31458\n",
            "Epoch 4/5 | Step 8010 | Loss: 2.9757 | LR: 8.11e-05 | Tokens/s: 31463\n",
            "Epoch 4/5 | Step 8020 | Loss: 2.8576 | LR: 8.07e-05 | Tokens/s: 31466\n",
            "Epoch 4/5 | Step 8030 | Loss: 2.8283 | LR: 8.04e-05 | Tokens/s: 31470\n",
            "Epoch 4/5 | Step 8040 | Loss: 2.9287 | LR: 8.00e-05 | Tokens/s: 31473\n",
            "Epoch 4/5 | Step 8050 | Loss: 2.8515 | LR: 7.97e-05 | Tokens/s: 31475\n",
            "Epoch 4/5 | Step 8060 | Loss: 3.1479 | LR: 7.94e-05 | Tokens/s: 31478\n",
            "Epoch 4/5 | Step 8070 | Loss: 2.9998 | LR: 7.90e-05 | Tokens/s: 31481\n",
            "Epoch 4/5 | Step 8080 | Loss: 2.7103 | LR: 7.87e-05 | Tokens/s: 31485\n",
            "Epoch 4/5 | Step 8090 | Loss: 3.2180 | LR: 7.83e-05 | Tokens/s: 31479\n",
            "Epoch 4/5 | Step 8100 | Loss: 2.6841 | LR: 7.80e-05 | Tokens/s: 31467\n",
            "Epoch 4/5 | Step 8110 | Loss: 2.7575 | LR: 7.77e-05 | Tokens/s: 31459\n",
            "Epoch 4/5 | Step 8120 | Loss: 2.2976 | LR: 7.73e-05 | Tokens/s: 31445\n",
            "Epoch 4/5 | Step 8130 | Loss: 2.4084 | LR: 7.70e-05 | Tokens/s: 31447\n",
            "Epoch 4/5 | Step 8140 | Loss: 2.6563 | LR: 7.66e-05 | Tokens/s: 31450\n",
            "Epoch 4/5 | Step 8150 | Loss: 2.8648 | LR: 7.63e-05 | Tokens/s: 31453\n",
            "Epoch 4/5 | Step 8160 | Loss: 2.7261 | LR: 7.60e-05 | Tokens/s: 31457\n",
            "Epoch 4/5 | Step 8170 | Loss: 2.9985 | LR: 7.56e-05 | Tokens/s: 31458\n",
            "Epoch 4/5 | Step 8180 | Loss: 2.7872 | LR: 7.53e-05 | Tokens/s: 31462\n",
            "Epoch 4/5 | Step 8190 | Loss: 2.7855 | LR: 7.50e-05 | Tokens/s: 31465\n",
            "Epoch 4/5 | Step 8200 | Loss: 2.7172 | LR: 7.46e-05 | Tokens/s: 31469\n",
            "Epoch 4/5 | Step 8210 | Loss: 2.6660 | LR: 7.43e-05 | Tokens/s: 31473\n",
            "Epoch 4/5 | Step 8220 | Loss: 2.5272 | LR: 7.40e-05 | Tokens/s: 31475\n",
            "Epoch 4/5 | Step 8230 | Loss: 2.6371 | LR: 7.36e-05 | Tokens/s: 31479\n",
            "Epoch 4/5 | Step 8240 | Loss: 2.4300 | LR: 7.33e-05 | Tokens/s: 31483\n",
            "Epoch 4/5 | Step 8250 | Loss: 2.7352 | LR: 7.30e-05 | Tokens/s: 31486\n",
            "Epoch 4/5 | Step 8260 | Loss: 2.8829 | LR: 7.26e-05 | Tokens/s: 31491\n",
            "Epoch 4/5 | Step 8270 | Loss: 2.5023 | LR: 7.23e-05 | Tokens/s: 31494\n",
            "Epoch 4/5 | Step 8280 | Loss: 3.0073 | LR: 7.20e-05 | Tokens/s: 31498\n",
            "Epoch 4/5 | Step 8290 | Loss: 3.6052 | LR: 7.16e-05 | Tokens/s: 31497\n",
            "Epoch 4/5 | Step 8300 | Loss: 3.2742 | LR: 7.13e-05 | Tokens/s: 31484\n",
            "Epoch 4/5 | Step 8310 | Loss: 3.2885 | LR: 7.10e-05 | Tokens/s: 31474\n",
            "Epoch 4/5 | Step 8320 | Loss: 2.6481 | LR: 7.07e-05 | Tokens/s: 31466\n",
            "Epoch 4/5 | Step 8330 | Loss: 3.3684 | LR: 7.03e-05 | Tokens/s: 31458\n",
            "Epoch 4/5 | Step 8340 | Loss: 3.1599 | LR: 7.00e-05 | Tokens/s: 31461\n",
            "Epoch 4/5 | Step 8350 | Loss: 3.3265 | LR: 6.97e-05 | Tokens/s: 31465\n",
            "Epoch 4/5 | Step 8360 | Loss: 2.5610 | LR: 6.93e-05 | Tokens/s: 31468\n",
            "Epoch 4/5 | Step 8370 | Loss: 2.9981 | LR: 6.90e-05 | Tokens/s: 31471\n",
            "Epoch 4/5 | Step 8380 | Loss: 3.1348 | LR: 6.87e-05 | Tokens/s: 31475\n",
            "Epoch 4/5 | Step 8390 | Loss: 3.1948 | LR: 6.84e-05 | Tokens/s: 31477\n",
            "Epoch 4/5 | Step 8400 | Loss: 2.8905 | LR: 6.80e-05 | Tokens/s: 31481\n",
            "Epoch 4/5 | Step 8410 | Loss: 2.7395 | LR: 6.77e-05 | Tokens/s: 31485\n",
            "Epoch 4/5 | Step 8420 | Loss: 2.6555 | LR: 6.74e-05 | Tokens/s: 31486\n",
            "Epoch 4/5 | Step 8430 | Loss: 2.3025 | LR: 6.71e-05 | Tokens/s: 31489\n",
            "Epoch 4/5 | Step 8440 | Loss: 2.6137 | LR: 6.68e-05 | Tokens/s: 31493\n",
            "Epoch 4/5 | Step 8450 | Loss: 2.5624 | LR: 6.64e-05 | Tokens/s: 31496\n",
            "Epoch 4/5 | Step 8460 | Loss: 2.8332 | LR: 6.61e-05 | Tokens/s: 31499\n",
            "Epoch 4/5 | Step 8470 | Loss: 2.7995 | LR: 6.58e-05 | Tokens/s: 31503\n",
            "Epoch 4/5 | Step 8480 | Loss: 2.5221 | LR: 6.55e-05 | Tokens/s: 31505\n",
            "Epoch 4/5 | Step 8490 | Loss: 2.5777 | LR: 6.52e-05 | Tokens/s: 31508\n",
            "Epoch 4/5 | Step 8500 | Loss: 2.5659 | LR: 6.48e-05 | Tokens/s: 31501\n",
            "Epoch 4/5 | Step 8510 | Loss: 2.6046 | LR: 6.45e-05 | Tokens/s: 31491\n",
            "Epoch 4/5 | Step 8520 | Loss: 2.7033 | LR: 6.42e-05 | Tokens/s: 31482\n",
            "Epoch 4/5 | Step 8530 | Loss: 2.5925 | LR: 6.39e-05 | Tokens/s: 31469\n",
            "Epoch 4/5 | Step 8540 | Loss: 3.0161 | LR: 6.36e-05 | Tokens/s: 31469\n",
            "Epoch 4/5 | Step 8550 | Loss: 2.6637 | LR: 6.33e-05 | Tokens/s: 31471\n",
            "Epoch 4/5 | Step 8560 | Loss: 2.4786 | LR: 6.29e-05 | Tokens/s: 31476\n",
            "Epoch 4/5 | Step 8570 | Loss: 2.3032 | LR: 6.26e-05 | Tokens/s: 31479\n",
            "Epoch 4/5 | Step 8580 | Loss: 2.5737 | LR: 6.23e-05 | Tokens/s: 31483\n",
            "Epoch 4/5 | Step 8590 | Loss: 2.6507 | LR: 6.20e-05 | Tokens/s: 31485\n",
            "Epoch 4/5 | Step 8600 | Loss: 2.4943 | LR: 6.17e-05 | Tokens/s: 31488\n",
            "Epoch 4/5 | Step 8610 | Loss: 2.5894 | LR: 6.14e-05 | Tokens/s: 31491\n",
            "Epoch 4/5 | Step 8620 | Loss: 2.5600 | LR: 6.11e-05 | Tokens/s: 31494\n",
            "Epoch 4/5 | Step 8630 | Loss: 2.6161 | LR: 6.08e-05 | Tokens/s: 31498\n",
            "Epoch 4/5 | Step 8640 | Loss: 2.6720 | LR: 6.04e-05 | Tokens/s: 31501\n",
            "Epoch 4/5 | Step 8650 | Loss: 2.5216 | LR: 6.01e-05 | Tokens/s: 31504\n",
            "Epoch 4/5 | Step 8660 | Loss: 2.3834 | LR: 5.98e-05 | Tokens/s: 31505\n",
            "Epoch 4/5 | Step 8670 | Loss: 2.4572 | LR: 5.95e-05 | Tokens/s: 31508\n",
            "Epoch 4/5 | Step 8680 | Loss: 2.4302 | LR: 5.92e-05 | Tokens/s: 31512\n",
            "Epoch 4/5 | Step 8690 | Loss: 2.4956 | LR: 5.89e-05 | Tokens/s: 31514\n",
            "Epoch 4/5 | Step 8700 | Loss: 2.3452 | LR: 5.86e-05 | Tokens/s: 31512\n",
            "Epoch 4/5 | Step 8710 | Loss: 2.3815 | LR: 5.83e-05 | Tokens/s: 31503\n",
            "Epoch 4/5 | Step 8720 | Loss: 2.5985 | LR: 5.80e-05 | Tokens/s: 31492\n",
            "Epoch 4/5 | Step 8730 | Loss: 2.4452 | LR: 5.77e-05 | Tokens/s: 31482\n",
            "Epoch 4/5 | Step 8740 | Loss: 3.2840 | LR: 5.74e-05 | Tokens/s: 31475\n",
            "Epoch 4/5 | Step 8750 | Loss: 2.8284 | LR: 5.71e-05 | Tokens/s: 31477\n",
            "Epoch 4/5 | Step 8760 | Loss: 3.0522 | LR: 5.68e-05 | Tokens/s: 31479\n",
            "Epoch 4/5 | Step 8770 | Loss: 3.0741 | LR: 5.65e-05 | Tokens/s: 31483\n",
            "Epoch 4/5 | Step 8780 | Loss: 2.8873 | LR: 5.62e-05 | Tokens/s: 31484\n",
            "Epoch 4/5 | Step 8790 | Loss: 2.9316 | LR: 5.59e-05 | Tokens/s: 31487\n",
            "Epoch 4/5 | Step 8800 | Loss: 3.0662 | LR: 5.56e-05 | Tokens/s: 31490\n",
            "Epoch 4/5 | Step 8810 | Loss: 2.6398 | LR: 5.53e-05 | Tokens/s: 31494\n",
            "Epoch 4/5 | Step 8820 | Loss: 2.6475 | LR: 5.50e-05 | Tokens/s: 31497\n",
            "Epoch 4/5 | Step 8830 | Loss: 2.6195 | LR: 5.47e-05 | Tokens/s: 31498\n",
            "Epoch 4/5 | Step 8840 | Loss: 2.7991 | LR: 5.44e-05 | Tokens/s: 31499\n",
            "Epoch 4/5 | Step 8850 | Loss: 2.8195 | LR: 5.41e-05 | Tokens/s: 31500\n",
            "Epoch 4/5 | Step 8860 | Loss: 2.4048 | LR: 5.38e-05 | Tokens/s: 31502\n",
            "Epoch 4/5 | Step 8870 | Loss: 2.7227 | LR: 5.35e-05 | Tokens/s: 31506\n",
            "Epoch 4/5 | Step 8880 | Loss: 3.0594 | LR: 5.32e-05 | Tokens/s: 31509\n",
            "Epoch 4/5 | Step 8890 | Loss: 2.7593 | LR: 5.29e-05 | Tokens/s: 31513\n",
            "Epoch 4/5 | Step 8900 | Loss: 2.9851 | LR: 5.26e-05 | Tokens/s: 31510\n",
            "Epoch 4/5 | Step 8910 | Loss: 2.5710 | LR: 5.23e-05 | Tokens/s: 31501\n",
            "Epoch 4/5 | Step 8920 | Loss: 2.7567 | LR: 5.20e-05 | Tokens/s: 31491\n",
            "Epoch 4/5 | Step 8930 | Loss: 2.4485 | LR: 5.17e-05 | Tokens/s: 31480\n",
            "Epoch 4/5 | Step 8940 | Loss: 2.5875 | LR: 5.14e-05 | Tokens/s: 31473\n",
            "Epoch 4/5 | Step 8950 | Loss: 2.3838 | LR: 5.11e-05 | Tokens/s: 31477\n",
            "Epoch 4/5 | Step 8960 | Loss: 2.4508 | LR: 5.09e-05 | Tokens/s: 31480\n",
            "Epoch 4/5 | Step 8970 | Loss: 2.3901 | LR: 5.06e-05 | Tokens/s: 31483\n",
            "Epoch 4/5 | Step 8980 | Loss: 2.1937 | LR: 5.03e-05 | Tokens/s: 31486\n",
            "Epoch 4/5 | Step 8990 | Loss: 2.8583 | LR: 5.00e-05 | Tokens/s: 31489\n",
            "Epoch 4/5 | Step 9000 | Loss: 3.0013 | LR: 4.97e-05 | Tokens/s: 31490\n",
            "Epoch 4/5 | Step 9010 | Loss: 2.6083 | LR: 4.94e-05 | Tokens/s: 31493\n",
            "Epoch 4/5 | Step 9020 | Loss: 2.8705 | LR: 4.91e-05 | Tokens/s: 31493\n",
            "Epoch 4/5 | Step 9030 | Loss: 2.6714 | LR: 4.88e-05 | Tokens/s: 31497\n",
            "Epoch 4/5 | Step 9040 | Loss: 2.5178 | LR: 4.86e-05 | Tokens/s: 31499\n",
            "Epoch 4/5 | Step 9050 | Loss: 3.0018 | LR: 4.83e-05 | Tokens/s: 31501\n",
            "Epoch 4/5 | Step 9060 | Loss: 3.0489 | LR: 4.80e-05 | Tokens/s: 31504\n",
            "Epoch 4/5 | Step 9070 | Loss: 3.0239 | LR: 4.77e-05 | Tokens/s: 31508\n",
            "Epoch 4/5 | Step 9080 | Loss: 2.7900 | LR: 4.74e-05 | Tokens/s: 31511\n",
            "Epoch 4/5 | Step 9090 | Loss: 2.6590 | LR: 4.71e-05 | Tokens/s: 31514\n",
            "Epoch 4/5 | Step 9100 | Loss: 2.9875 | LR: 4.69e-05 | Tokens/s: 31518\n",
            "Epoch 4/5 | Step 9110 | Loss: 2.5771 | LR: 4.66e-05 | Tokens/s: 31510\n",
            "Epoch 4/5 | Step 9120 | Loss: 3.0189 | LR: 4.63e-05 | Tokens/s: 31501\n",
            "Epoch 4/5 | Step 9130 | Loss: 2.5333 | LR: 4.60e-05 | Tokens/s: 31494\n",
            "Epoch 4/5 | Step 9140 | Loss: 3.0231 | LR: 4.58e-05 | Tokens/s: 31479\n",
            "Epoch 4/5 | Step 9150 | Loss: 2.4984 | LR: 4.55e-05 | Tokens/s: 31482\n",
            "Epoch 4/5 | Step 9160 | Loss: 3.0687 | LR: 4.52e-05 | Tokens/s: 31485\n",
            "Epoch 4/5 | Step 9170 | Loss: 2.7312 | LR: 4.49e-05 | Tokens/s: 31488\n",
            "Epoch 4/5 | Step 9180 | Loss: 2.8515 | LR: 4.47e-05 | Tokens/s: 31491\n",
            "Epoch 4/5 | Step 9190 | Loss: 3.5946 | LR: 4.44e-05 | Tokens/s: 31494\n",
            "Epoch 4/5 | Step 9200 | Loss: 2.7611 | LR: 4.41e-05 | Tokens/s: 31498\n",
            "Epoch 4/5 | Step 9210 | Loss: 2.7685 | LR: 4.38e-05 | Tokens/s: 31501\n",
            "Epoch 4/5 | Step 9220 | Loss: 2.7146 | LR: 4.36e-05 | Tokens/s: 31504\n",
            "Epoch 4/5 | Step 9230 | Loss: 2.6246 | LR: 4.33e-05 | Tokens/s: 31507\n",
            "Epoch 4/5 | Step 9240 | Loss: 3.2696 | LR: 4.30e-05 | Tokens/s: 31510\n",
            "Epoch 4/5 | Step 9250 | Loss: 3.8676 | LR: 4.27e-05 | Tokens/s: 31513\n",
            "Epoch 4/5 | Step 9260 | Loss: 2.4425 | LR: 4.25e-05 | Tokens/s: 31515\n",
            "Epoch 4/5 | Step 9270 | Loss: 3.6378 | LR: 4.22e-05 | Tokens/s: 31518\n",
            "Epoch 4/5 | Step 9280 | Loss: 2.6686 | LR: 4.19e-05 | Tokens/s: 31521\n",
            "Epoch 4/5 | Step 9290 | Loss: 2.4374 | LR: 4.17e-05 | Tokens/s: 31522\n",
            "Epoch 4/5 | Step 9300 | Loss: 3.7314 | LR: 4.14e-05 | Tokens/s: 31526\n",
            "Epoch 4/5 | Step 9310 | Loss: 2.8858 | LR: 4.11e-05 | Tokens/s: 31522\n",
            "Epoch 4/5 | Step 9320 | Loss: 3.4278 | LR: 4.09e-05 | Tokens/s: 31514\n",
            "Epoch 4/5 | Step 9330 | Loss: 2.6294 | LR: 4.06e-05 | Tokens/s: 31503\n",
            "Epoch 4/5 | Step 9340 | Loss: 3.1802 | LR: 4.04e-05 | Tokens/s: 31494\n",
            "Epoch 4/5 | Step 9350 | Loss: 2.7656 | LR: 4.01e-05 | Tokens/s: 31489\n",
            "Epoch 4/5 | Step 9360 | Loss: 2.5610 | LR: 3.98e-05 | Tokens/s: 31492\n",
            "Epoch 4/5 | Step 9370 | Loss: 3.5462 | LR: 3.96e-05 | Tokens/s: 31496\n",
            "Epoch 4/5 | Step 9380 | Loss: 3.5859 | LR: 3.93e-05 | Tokens/s: 31497\n",
            "Epoch 4/5 | Step 9390 | Loss: 3.4465 | LR: 3.91e-05 | Tokens/s: 31500\n",
            "Epoch 4/5 | Step 9400 | Loss: 2.9119 | LR: 3.88e-05 | Tokens/s: 31501\n",
            "Epoch 4/5 | Step 9410 | Loss: 3.7007 | LR: 3.85e-05 | Tokens/s: 31504\n",
            "Epoch 4/5 | Step 9420 | Loss: 3.4499 | LR: 3.83e-05 | Tokens/s: 31508\n",
            "Epoch 4/5 | Step 9430 | Loss: 2.9205 | LR: 3.80e-05 | Tokens/s: 31510\n",
            "Epoch 4/5 | Step 9440 | Loss: 2.8339 | LR: 3.78e-05 | Tokens/s: 31514\n",
            "Epoch 4/5 | Step 9450 | Loss: 2.8398 | LR: 3.75e-05 | Tokens/s: 31516\n",
            "Epoch 4/5 | Step 9460 | Loss: 2.7935 | LR: 3.73e-05 | Tokens/s: 31520\n",
            "Epoch 4/5 | Step 9470 | Loss: 2.9573 | LR: 3.70e-05 | Tokens/s: 31523\n",
            "Epoch 4/5 | Step 9480 | Loss: 3.0326 | LR: 3.68e-05 | Tokens/s: 31525\n",
            "Epoch 4/5 | Step 9490 | Loss: 2.9801 | LR: 3.65e-05 | Tokens/s: 31527\n",
            "Epoch 4/5 | Step 9500 | Loss: 3.0394 | LR: 3.63e-05 | Tokens/s: 31525\n",
            "Epoch 4/5 | Step 9510 | Loss: 3.4911 | LR: 3.60e-05 | Tokens/s: 31521\n",
            "Epoch 4/5 | Step 9520 | Loss: 1.9897 | LR: 3.58e-05 | Tokens/s: 31508\n",
            "Epoch 4/5 | Step 9530 | Loss: 2.8360 | LR: 3.55e-05 | Tokens/s: 31496\n",
            "Epoch 4/5 | Step 9540 | Loss: 3.0957 | LR: 3.53e-05 | Tokens/s: 31484\n",
            "Epoch 4/5 | Step 9550 | Loss: 3.2171 | LR: 3.50e-05 | Tokens/s: 31473\n",
            "Epoch 4/5 | Step 9560 | Loss: 2.8911 | LR: 3.48e-05 | Tokens/s: 31477\n",
            "Epoch 4/5 | Step 9570 | Loss: 3.1076 | LR: 3.45e-05 | Tokens/s: 31479\n",
            "Epoch 4/5 | Step 9580 | Loss: 4.6249 | LR: 3.43e-05 | Tokens/s: 31482\n",
            "Epoch 4/5 | Step 9590 | Loss: 2.9931 | LR: 3.40e-05 | Tokens/s: 31485\n",
            "Epoch 4/5 | Step 9600 | Loss: 2.7871 | LR: 3.38e-05 | Tokens/s: 31486\n",
            "Epoch 4/5 | Step 9610 | Loss: 3.1097 | LR: 3.35e-05 | Tokens/s: 31488\n",
            "Epoch 4/5 | Step 9620 | Loss: 4.8651 | LR: 3.33e-05 | Tokens/s: 31489\n",
            "Epoch 4/5 | Step 9630 | Loss: 4.3637 | LR: 3.31e-05 | Tokens/s: 31488\n",
            "Epoch 4/5 | Step 9640 | Loss: 4.4786 | LR: 3.28e-05 | Tokens/s: 31491\n",
            "Epoch 4/5 | Step 9650 | Loss: 4.4689 | LR: 3.26e-05 | Tokens/s: 31491\n",
            "Epoch 4/5 | Step 9660 | Loss: 4.1238 | LR: 3.23e-05 | Tokens/s: 31493\n",
            "Epoch 4/5 | Step 9670 | Loss: 4.2592 | LR: 3.21e-05 | Tokens/s: 31493\n",
            "Epoch 4/5 | Step 9680 | Loss: 4.0585 | LR: 3.19e-05 | Tokens/s: 31493\n",
            "Epoch 4/5 | Step 9690 | Loss: 4.2092 | LR: 3.16e-05 | Tokens/s: 31494\n",
            "Epoch 4/5 | Step 9700 | Loss: 4.0980 | LR: 3.14e-05 | Tokens/s: 31496\n",
            "Epoch 4/5 | Step 9710 | Loss: 4.4698 | LR: 3.12e-05 | Tokens/s: 31490\n",
            "Epoch 4/5 | Step 9720 | Loss: 4.1809 | LR: 3.09e-05 | Tokens/s: 31481\n",
            "Epoch 4/5 | Step 9730 | Loss: 3.9537 | LR: 3.07e-05 | Tokens/s: 31471\n",
            "Epoch 4/5 | Step 9740 | Loss: 3.8251 | LR: 3.05e-05 | Tokens/s: 31461\n",
            "Epoch 4/5 | Step 9750 | Loss: 4.3528 | LR: 3.02e-05 | Tokens/s: 31455\n",
            "Epoch 4/5 | Step 9760 | Loss: 4.0673 | LR: 3.00e-05 | Tokens/s: 31457\n",
            "\n",
            "========================================\n",
            "Epoch 4 complete | Average Loss: 3.0188\n",
            "========================================\n",
            "\n",
            "Epoch 5/5 | Step 9770 | Loss: 4.7064 | LR: 2.98e-05 | Tokens/s: 31457\n",
            "Epoch 5/5 | Step 9780 | Loss: 4.6567 | LR: 2.96e-05 | Tokens/s: 31461\n",
            "Epoch 5/5 | Step 9790 | Loss: 4.6319 | LR: 2.93e-05 | Tokens/s: 31463\n",
            "Epoch 5/5 | Step 9800 | Loss: 4.6929 | LR: 2.91e-05 | Tokens/s: 31465\n",
            "Epoch 5/5 | Step 9810 | Loss: 3.3029 | LR: 2.89e-05 | Tokens/s: 31468\n",
            "Epoch 5/5 | Step 9820 | Loss: 3.0539 | LR: 2.86e-05 | Tokens/s: 31471\n",
            "Epoch 5/5 | Step 9830 | Loss: 3.0209 | LR: 2.84e-05 | Tokens/s: 31473\n",
            "Epoch 5/5 | Step 9840 | Loss: 2.9353 | LR: 2.82e-05 | Tokens/s: 31475\n",
            "Epoch 5/5 | Step 9850 | Loss: 3.0354 | LR: 2.80e-05 | Tokens/s: 31478\n",
            "Epoch 5/5 | Step 9860 | Loss: 3.0990 | LR: 2.78e-05 | Tokens/s: 31479\n",
            "Epoch 5/5 | Step 9870 | Loss: 3.1013 | LR: 2.75e-05 | Tokens/s: 31479\n",
            "Epoch 5/5 | Step 9880 | Loss: 2.6196 | LR: 2.73e-05 | Tokens/s: 31482\n",
            "Epoch 5/5 | Step 9890 | Loss: 3.2458 | LR: 2.71e-05 | Tokens/s: 31482\n",
            "Epoch 5/5 | Step 9900 | Loss: 3.7370 | LR: 2.69e-05 | Tokens/s: 31484\n",
            "Epoch 5/5 | Step 9910 | Loss: 3.0756 | LR: 2.67e-05 | Tokens/s: 31478\n",
            "Epoch 5/5 | Step 9920 | Loss: 2.9806 | LR: 2.64e-05 | Tokens/s: 31469\n",
            "Epoch 5/5 | Step 9930 | Loss: 3.1144 | LR: 2.62e-05 | Tokens/s: 31458\n",
            "Epoch 5/5 | Step 9940 | Loss: 2.9058 | LR: 2.60e-05 | Tokens/s: 31447\n",
            "Epoch 5/5 | Step 9950 | Loss: 3.2347 | LR: 2.58e-05 | Tokens/s: 31446\n",
            "Epoch 5/5 | Step 9960 | Loss: 3.1299 | LR: 2.56e-05 | Tokens/s: 31446\n",
            "Epoch 5/5 | Step 9970 | Loss: 3.5139 | LR: 2.54e-05 | Tokens/s: 31448\n",
            "Epoch 5/5 | Step 9980 | Loss: 2.7663 | LR: 2.52e-05 | Tokens/s: 31450\n",
            "Epoch 5/5 | Step 9990 | Loss: 2.9198 | LR: 2.49e-05 | Tokens/s: 31451\n",
            "Epoch 5/5 | Step 10000 | Loss: 3.0389 | LR: 2.47e-05 | Tokens/s: 31452\n",
            "Epoch 5/5 | Step 10010 | Loss: 2.8571 | LR: 2.45e-05 | Tokens/s: 31453\n",
            "Epoch 5/5 | Step 10020 | Loss: 2.9364 | LR: 2.43e-05 | Tokens/s: 31454\n",
            "Epoch 5/5 | Step 10030 | Loss: 2.7476 | LR: 2.41e-05 | Tokens/s: 31456\n",
            "Epoch 5/5 | Step 10040 | Loss: 2.8865 | LR: 2.39e-05 | Tokens/s: 31458\n",
            "Epoch 5/5 | Step 10050 | Loss: 2.7432 | LR: 2.37e-05 | Tokens/s: 31460\n",
            "Epoch 5/5 | Step 10060 | Loss: 2.7970 | LR: 2.35e-05 | Tokens/s: 31462\n",
            "Epoch 5/5 | Step 10070 | Loss: 2.9124 | LR: 2.33e-05 | Tokens/s: 31464\n",
            "Epoch 5/5 | Step 10080 | Loss: 2.9591 | LR: 2.31e-05 | Tokens/s: 31465\n",
            "Epoch 5/5 | Step 10090 | Loss: 2.7527 | LR: 2.29e-05 | Tokens/s: 31468\n",
            "Epoch 5/5 | Step 10100 | Loss: 3.0199 | LR: 2.27e-05 | Tokens/s: 31471\n",
            "Epoch 5/5 | Step 10110 | Loss: 2.7223 | LR: 2.25e-05 | Tokens/s: 31462\n",
            "Epoch 5/5 | Step 10120 | Loss: 2.8414 | LR: 2.23e-05 | Tokens/s: 31451\n",
            "Epoch 5/5 | Step 10130 | Loss: 2.9985 | LR: 2.21e-05 | Tokens/s: 31443\n",
            "Epoch 5/5 | Step 10140 | Loss: 2.6224 | LR: 2.19e-05 | Tokens/s: 31431\n",
            "Epoch 5/5 | Step 10150 | Loss: 2.7222 | LR: 2.17e-05 | Tokens/s: 31431\n",
            "Epoch 5/5 | Step 10160 | Loss: 2.8171 | LR: 2.15e-05 | Tokens/s: 31433\n",
            "Epoch 5/5 | Step 10170 | Loss: 2.8214 | LR: 2.13e-05 | Tokens/s: 31435\n",
            "Epoch 5/5 | Step 10180 | Loss: 2.4931 | LR: 2.11e-05 | Tokens/s: 31437\n",
            "Epoch 5/5 | Step 10190 | Loss: 2.9427 | LR: 2.09e-05 | Tokens/s: 31438\n",
            "Epoch 5/5 | Step 10200 | Loss: 2.6834 | LR: 2.07e-05 | Tokens/s: 31439\n",
            "Epoch 5/5 | Step 10210 | Loss: 2.5381 | LR: 2.05e-05 | Tokens/s: 31442\n",
            "Epoch 5/5 | Step 10220 | Loss: 2.4880 | LR: 2.03e-05 | Tokens/s: 31444\n",
            "Epoch 5/5 | Step 10230 | Loss: 3.8195 | LR: 2.01e-05 | Tokens/s: 31444\n",
            "Epoch 5/5 | Step 10240 | Loss: 3.4237 | LR: 1.99e-05 | Tokens/s: 31447\n",
            "Epoch 5/5 | Step 10250 | Loss: 2.5418 | LR: 1.98e-05 | Tokens/s: 31449\n",
            "Epoch 5/5 | Step 10260 | Loss: 3.1030 | LR: 1.96e-05 | Tokens/s: 31451\n",
            "Epoch 5/5 | Step 10270 | Loss: 3.0358 | LR: 1.94e-05 | Tokens/s: 31453\n",
            "Epoch 5/5 | Step 10280 | Loss: 2.9828 | LR: 1.92e-05 | Tokens/s: 31454\n",
            "Epoch 5/5 | Step 10290 | Loss: 2.8094 | LR: 1.90e-05 | Tokens/s: 31456\n",
            "Epoch 5/5 | Step 10300 | Loss: 2.8000 | LR: 1.88e-05 | Tokens/s: 31458\n",
            "Epoch 5/5 | Step 10310 | Loss: 3.0852 | LR: 1.86e-05 | Tokens/s: 31452\n",
            "Epoch 5/5 | Step 10320 | Loss: 2.7942 | LR: 1.85e-05 | Tokens/s: 31444\n",
            "Epoch 5/5 | Step 10330 | Loss: 5.3093 | LR: 1.83e-05 | Tokens/s: 31436\n",
            "Epoch 5/5 | Step 10340 | Loss: 3.2850 | LR: 1.81e-05 | Tokens/s: 31424\n",
            "Epoch 5/5 | Step 10350 | Loss: 1.9877 | LR: 1.79e-05 | Tokens/s: 31421\n",
            "Epoch 5/5 | Step 10360 | Loss: 2.8889 | LR: 1.77e-05 | Tokens/s: 31423\n",
            "Epoch 5/5 | Step 10370 | Loss: 3.0186 | LR: 1.76e-05 | Tokens/s: 31425\n",
            "Epoch 5/5 | Step 10380 | Loss: 3.0350 | LR: 1.74e-05 | Tokens/s: 31427\n",
            "Epoch 5/5 | Step 10390 | Loss: 2.7293 | LR: 1.72e-05 | Tokens/s: 31429\n",
            "Epoch 5/5 | Step 10400 | Loss: 2.9587 | LR: 1.70e-05 | Tokens/s: 31431\n",
            "Epoch 5/5 | Step 10410 | Loss: 3.4707 | LR: 1.69e-05 | Tokens/s: 31433\n",
            "Epoch 5/5 | Step 10420 | Loss: 2.9389 | LR: 1.67e-05 | Tokens/s: 31435\n",
            "Epoch 5/5 | Step 10430 | Loss: 3.1957 | LR: 1.65e-05 | Tokens/s: 31438\n",
            "Epoch 5/5 | Step 10440 | Loss: 3.0965 | LR: 1.63e-05 | Tokens/s: 31440\n",
            "Epoch 5/5 | Step 10450 | Loss: 2.9505 | LR: 1.62e-05 | Tokens/s: 31442\n",
            "Epoch 5/5 | Step 10460 | Loss: 2.8938 | LR: 1.60e-05 | Tokens/s: 31445\n",
            "Epoch 5/5 | Step 10470 | Loss: 2.8047 | LR: 1.58e-05 | Tokens/s: 31445\n",
            "Epoch 5/5 | Step 10480 | Loss: 2.8989 | LR: 1.57e-05 | Tokens/s: 31448\n",
            "Epoch 5/5 | Step 10490 | Loss: 2.8224 | LR: 1.55e-05 | Tokens/s: 31450\n",
            "Epoch 5/5 | Step 10500 | Loss: 3.0713 | LR: 1.53e-05 | Tokens/s: 31452\n",
            "Epoch 5/5 | Step 10510 | Loss: 2.9042 | LR: 1.52e-05 | Tokens/s: 31448\n",
            "Epoch 5/5 | Step 10520 | Loss: 2.6497 | LR: 1.50e-05 | Tokens/s: 31440\n",
            "Epoch 5/5 | Step 10530 | Loss: 3.1766 | LR: 1.48e-05 | Tokens/s: 31431\n",
            "Epoch 5/5 | Step 10540 | Loss: 2.6736 | LR: 1.47e-05 | Tokens/s: 31422\n",
            "Epoch 5/5 | Step 10550 | Loss: 2.7241 | LR: 1.45e-05 | Tokens/s: 31420\n",
            "Epoch 5/5 | Step 10560 | Loss: 2.2944 | LR: 1.43e-05 | Tokens/s: 31423\n",
            "Epoch 5/5 | Step 10570 | Loss: 2.3928 | LR: 1.42e-05 | Tokens/s: 31425\n",
            "Epoch 5/5 | Step 10580 | Loss: 2.6572 | LR: 1.40e-05 | Tokens/s: 31427\n",
            "Epoch 5/5 | Step 10590 | Loss: 2.8209 | LR: 1.39e-05 | Tokens/s: 31427\n",
            "Epoch 5/5 | Step 10600 | Loss: 2.6985 | LR: 1.37e-05 | Tokens/s: 31429\n",
            "Epoch 5/5 | Step 10610 | Loss: 3.0275 | LR: 1.36e-05 | Tokens/s: 31430\n",
            "Epoch 5/5 | Step 10620 | Loss: 2.7861 | LR: 1.34e-05 | Tokens/s: 31433\n",
            "Epoch 5/5 | Step 10630 | Loss: 2.7322 | LR: 1.32e-05 | Tokens/s: 31435\n",
            "Epoch 5/5 | Step 10640 | Loss: 2.6948 | LR: 1.31e-05 | Tokens/s: 31437\n",
            "Epoch 5/5 | Step 10650 | Loss: 2.6490 | LR: 1.29e-05 | Tokens/s: 31439\n",
            "Epoch 5/5 | Step 10660 | Loss: 2.5338 | LR: 1.28e-05 | Tokens/s: 31442\n",
            "Epoch 5/5 | Step 10670 | Loss: 2.6572 | LR: 1.26e-05 | Tokens/s: 31444\n",
            "Epoch 5/5 | Step 10680 | Loss: 2.4106 | LR: 1.25e-05 | Tokens/s: 31446\n",
            "Epoch 5/5 | Step 10690 | Loss: 2.7468 | LR: 1.23e-05 | Tokens/s: 31449\n",
            "Epoch 5/5 | Step 10700 | Loss: 2.8759 | LR: 1.22e-05 | Tokens/s: 31451\n",
            "Epoch 5/5 | Step 10710 | Loss: 2.4645 | LR: 1.20e-05 | Tokens/s: 31450\n",
            "Epoch 5/5 | Step 10720 | Loss: 2.9853 | LR: 1.19e-05 | Tokens/s: 31438\n",
            "Epoch 5/5 | Step 10730 | Loss: 3.5008 | LR: 1.18e-05 | Tokens/s: 31428\n",
            "Epoch 5/5 | Step 10740 | Loss: 3.2046 | LR: 1.16e-05 | Tokens/s: 31419\n",
            "Epoch 5/5 | Step 10750 | Loss: 3.3306 | LR: 1.15e-05 | Tokens/s: 31415\n",
            "Epoch 5/5 | Step 10760 | Loss: 2.6936 | LR: 1.13e-05 | Tokens/s: 31416\n",
            "Epoch 5/5 | Step 10770 | Loss: 3.4230 | LR: 1.12e-05 | Tokens/s: 31417\n",
            "Epoch 5/5 | Step 10780 | Loss: 3.2140 | LR: 1.10e-05 | Tokens/s: 31419\n",
            "Epoch 5/5 | Step 10790 | Loss: 3.4204 | LR: 1.09e-05 | Tokens/s: 31421\n",
            "Epoch 5/5 | Step 10800 | Loss: 2.5517 | LR: 1.08e-05 | Tokens/s: 31422\n",
            "Epoch 5/5 | Step 10810 | Loss: 3.0347 | LR: 1.06e-05 | Tokens/s: 31425\n",
            "Epoch 5/5 | Step 10820 | Loss: 3.1774 | LR: 1.05e-05 | Tokens/s: 31427\n",
            "Epoch 5/5 | Step 10830 | Loss: 3.1082 | LR: 1.04e-05 | Tokens/s: 31429\n",
            "Epoch 5/5 | Step 10840 | Loss: 2.8599 | LR: 1.02e-05 | Tokens/s: 31429\n",
            "Epoch 5/5 | Step 10850 | Loss: 2.6604 | LR: 1.01e-05 | Tokens/s: 31431\n",
            "Epoch 5/5 | Step 10860 | Loss: 2.6662 | LR: 9.96e-06 | Tokens/s: 31434\n",
            "Epoch 5/5 | Step 10870 | Loss: 2.2720 | LR: 9.83e-06 | Tokens/s: 31435\n",
            "Epoch 5/5 | Step 10880 | Loss: 2.6416 | LR: 9.69e-06 | Tokens/s: 31438\n",
            "Epoch 5/5 | Step 10890 | Loss: 2.5502 | LR: 9.56e-06 | Tokens/s: 31438\n",
            "Epoch 5/5 | Step 10900 | Loss: 2.8379 | LR: 9.44e-06 | Tokens/s: 31441\n",
            "Epoch 5/5 | Step 10910 | Loss: 2.7765 | LR: 9.31e-06 | Tokens/s: 31437\n",
            "Epoch 5/5 | Step 10920 | Loss: 2.5809 | LR: 9.18e-06 | Tokens/s: 31429\n",
            "Epoch 5/5 | Step 10930 | Loss: 2.5913 | LR: 9.05e-06 | Tokens/s: 31419\n",
            "Epoch 5/5 | Step 10940 | Loss: 2.5696 | LR: 8.93e-06 | Tokens/s: 31409\n",
            "Epoch 5/5 | Step 10950 | Loss: 2.5916 | LR: 8.80e-06 | Tokens/s: 31405\n",
            "Epoch 5/5 | Step 10960 | Loss: 2.7295 | LR: 8.68e-06 | Tokens/s: 31404\n",
            "Epoch 5/5 | Step 10970 | Loss: 2.4308 | LR: 8.56e-06 | Tokens/s: 31406\n",
            "Epoch 5/5 | Step 10980 | Loss: 2.9963 | LR: 8.44e-06 | Tokens/s: 31408\n",
            "Epoch 5/5 | Step 10990 | Loss: 2.6597 | LR: 8.32e-06 | Tokens/s: 31408\n",
            "Epoch 5/5 | Step 11000 | Loss: 2.4428 | LR: 8.20e-06 | Tokens/s: 31410\n",
            "Epoch 5/5 | Step 11010 | Loss: 2.3653 | LR: 8.08e-06 | Tokens/s: 31412\n",
            "Epoch 5/5 | Step 11020 | Loss: 2.6010 | LR: 7.96e-06 | Tokens/s: 31415\n",
            "Epoch 5/5 | Step 11030 | Loss: 2.6870 | LR: 7.84e-06 | Tokens/s: 31417\n",
            "Epoch 5/5 | Step 11040 | Loss: 2.5086 | LR: 7.73e-06 | Tokens/s: 31419\n",
            "Epoch 5/5 | Step 11050 | Loss: 2.5911 | LR: 7.61e-06 | Tokens/s: 31421\n",
            "Epoch 5/5 | Step 11060 | Loss: 2.5996 | LR: 7.50e-06 | Tokens/s: 31423\n",
            "Epoch 5/5 | Step 11070 | Loss: 2.6301 | LR: 7.39e-06 | Tokens/s: 31426\n",
            "Epoch 5/5 | Step 11080 | Loss: 2.6928 | LR: 7.28e-06 | Tokens/s: 31425\n",
            "Epoch 5/5 | Step 11090 | Loss: 2.5648 | LR: 7.17e-06 | Tokens/s: 31426\n",
            "Epoch 5/5 | Step 11100 | Loss: 2.4255 | LR: 7.06e-06 | Tokens/s: 31429\n",
            "Epoch 5/5 | Step 11110 | Loss: 2.4626 | LR: 6.95e-06 | Tokens/s: 31426\n",
            "Epoch 5/5 | Step 11120 | Loss: 2.4505 | LR: 6.84e-06 | Tokens/s: 31418\n",
            "Epoch 5/5 | Step 11130 | Loss: 2.5791 | LR: 6.73e-06 | Tokens/s: 31409\n",
            "Epoch 5/5 | Step 11140 | Loss: 2.3962 | LR: 6.63e-06 | Tokens/s: 31398\n",
            "Epoch 5/5 | Step 11150 | Loss: 2.4043 | LR: 6.52e-06 | Tokens/s: 31396\n",
            "Epoch 5/5 | Step 11160 | Loss: 2.6401 | LR: 6.42e-06 | Tokens/s: 31398\n",
            "Epoch 5/5 | Step 11170 | Loss: 2.5378 | LR: 6.31e-06 | Tokens/s: 31400\n",
            "Epoch 5/5 | Step 11180 | Loss: 3.2870 | LR: 6.21e-06 | Tokens/s: 31402\n",
            "Epoch 5/5 | Step 11190 | Loss: 2.8264 | LR: 6.11e-06 | Tokens/s: 31404\n",
            "Epoch 5/5 | Step 11200 | Loss: 3.0447 | LR: 6.01e-06 | Tokens/s: 31404\n",
            "Epoch 5/5 | Step 11210 | Loss: 3.0454 | LR: 5.91e-06 | Tokens/s: 31406\n",
            "Epoch 5/5 | Step 11220 | Loss: 2.8446 | LR: 5.81e-06 | Tokens/s: 31407\n",
            "Epoch 5/5 | Step 11230 | Loss: 2.9629 | LR: 5.72e-06 | Tokens/s: 31409\n",
            "Epoch 5/5 | Step 11240 | Loss: 3.0625 | LR: 5.62e-06 | Tokens/s: 31411\n",
            "Epoch 5/5 | Step 11250 | Loss: 2.7531 | LR: 5.52e-06 | Tokens/s: 31412\n",
            "Epoch 5/5 | Step 11260 | Loss: 2.6767 | LR: 5.43e-06 | Tokens/s: 31415\n",
            "Epoch 5/5 | Step 11270 | Loss: 2.6391 | LR: 5.34e-06 | Tokens/s: 31417\n",
            "Epoch 5/5 | Step 11280 | Loss: 2.8411 | LR: 5.24e-06 | Tokens/s: 31420\n",
            "Epoch 5/5 | Step 11290 | Loss: 2.9212 | LR: 5.15e-06 | Tokens/s: 31423\n",
            "Epoch 5/5 | Step 11300 | Loss: 2.4265 | LR: 5.06e-06 | Tokens/s: 31425\n",
            "Epoch 5/5 | Step 11310 | Loss: 2.7650 | LR: 4.97e-06 | Tokens/s: 31423\n",
            "Epoch 5/5 | Step 11320 | Loss: 3.0743 | LR: 4.89e-06 | Tokens/s: 31414\n",
            "Epoch 5/5 | Step 11330 | Loss: 2.7975 | LR: 4.80e-06 | Tokens/s: 31405\n",
            "Epoch 5/5 | Step 11340 | Loss: 2.9871 | LR: 4.71e-06 | Tokens/s: 31398\n",
            "Epoch 5/5 | Step 11350 | Loss: 2.5926 | LR: 4.63e-06 | Tokens/s: 31394\n",
            "Epoch 5/5 | Step 11360 | Loss: 2.7640 | LR: 4.54e-06 | Tokens/s: 31395\n",
            "Epoch 5/5 | Step 11370 | Loss: 2.3863 | LR: 4.46e-06 | Tokens/s: 31398\n",
            "Epoch 5/5 | Step 11380 | Loss: 2.5706 | LR: 4.38e-06 | Tokens/s: 31400\n",
            "Epoch 5/5 | Step 11390 | Loss: 2.3661 | LR: 4.29e-06 | Tokens/s: 31403\n",
            "Epoch 5/5 | Step 11400 | Loss: 2.4276 | LR: 4.21e-06 | Tokens/s: 31405\n",
            "Epoch 5/5 | Step 11410 | Loss: 2.4402 | LR: 4.13e-06 | Tokens/s: 31407\n",
            "Epoch 5/5 | Step 11420 | Loss: 2.2629 | LR: 4.06e-06 | Tokens/s: 31409\n",
            "Epoch 5/5 | Step 11430 | Loss: 2.8220 | LR: 3.98e-06 | Tokens/s: 31411\n",
            "Epoch 5/5 | Step 11440 | Loss: 3.0275 | LR: 3.90e-06 | Tokens/s: 31412\n",
            "Epoch 5/5 | Step 11450 | Loss: 2.6367 | LR: 3.83e-06 | Tokens/s: 31415\n",
            "Epoch 5/5 | Step 11460 | Loss: 2.8499 | LR: 3.75e-06 | Tokens/s: 31417\n",
            "Epoch 5/5 | Step 11470 | Loss: 2.6910 | LR: 3.68e-06 | Tokens/s: 31420\n",
            "Epoch 5/5 | Step 11480 | Loss: 2.5782 | LR: 3.60e-06 | Tokens/s: 31422\n",
            "Epoch 5/5 | Step 11490 | Loss: 2.9980 | LR: 3.53e-06 | Tokens/s: 31424\n",
            "Epoch 5/5 | Step 11500 | Loss: 3.0276 | LR: 3.46e-06 | Tokens/s: 31426\n",
            "Epoch 5/5 | Step 11510 | Loss: 2.9777 | LR: 3.39e-06 | Tokens/s: 31428\n",
            "Epoch 5/5 | Step 11520 | Loss: 2.7534 | LR: 3.32e-06 | Tokens/s: 31421\n",
            "Epoch 5/5 | Step 11530 | Loss: 2.6671 | LR: 3.26e-06 | Tokens/s: 31414\n",
            "Epoch 5/5 | Step 11540 | Loss: 2.9153 | LR: 3.19e-06 | Tokens/s: 31408\n",
            "Epoch 5/5 | Step 11550 | Loss: 2.6177 | LR: 3.12e-06 | Tokens/s: 31401\n",
            "Epoch 5/5 | Step 11560 | Loss: 3.0349 | LR: 3.06e-06 | Tokens/s: 31401\n",
            "Epoch 5/5 | Step 11570 | Loss: 2.5535 | LR: 3.00e-06 | Tokens/s: 31403\n",
            "Epoch 5/5 | Step 11580 | Loss: 2.9845 | LR: 2.93e-06 | Tokens/s: 31405\n",
            "Epoch 5/5 | Step 11590 | Loss: 2.5373 | LR: 2.87e-06 | Tokens/s: 31409\n",
            "Epoch 5/5 | Step 11600 | Loss: 3.0897 | LR: 2.81e-06 | Tokens/s: 31411\n",
            "Epoch 5/5 | Step 11610 | Loss: 2.7028 | LR: 2.75e-06 | Tokens/s: 31414\n",
            "Epoch 5/5 | Step 11620 | Loss: 2.8977 | LR: 2.69e-06 | Tokens/s: 31416\n",
            "Epoch 5/5 | Step 11630 | Loss: 3.5584 | LR: 2.63e-06 | Tokens/s: 31419\n",
            "Epoch 5/5 | Step 11640 | Loss: 2.8510 | LR: 2.58e-06 | Tokens/s: 31421\n",
            "Epoch 5/5 | Step 11650 | Loss: 2.6540 | LR: 2.52e-06 | Tokens/s: 31424\n",
            "Epoch 5/5 | Step 11660 | Loss: 2.6101 | LR: 2.47e-06 | Tokens/s: 31426\n",
            "Epoch 5/5 | Step 11670 | Loss: 2.6448 | LR: 2.41e-06 | Tokens/s: 31429\n",
            "Epoch 5/5 | Step 11680 | Loss: 3.2246 | LR: 2.36e-06 | Tokens/s: 31430\n",
            "Epoch 5/5 | Step 11690 | Loss: 3.8691 | LR: 2.31e-06 | Tokens/s: 31431\n",
            "Epoch 5/5 | Step 11700 | Loss: 2.4061 | LR: 2.26e-06 | Tokens/s: 31434\n",
            "Epoch 5/5 | Step 11710 | Loss: 3.7184 | LR: 2.21e-06 | Tokens/s: 31437\n",
            "Epoch 5/5 | Step 11720 | Loss: 2.6669 | LR: 2.16e-06 | Tokens/s: 31433\n",
            "Epoch 5/5 | Step 11730 | Loss: 2.5463 | LR: 2.11e-06 | Tokens/s: 31425\n",
            "Epoch 5/5 | Step 11740 | Loss: 3.7949 | LR: 2.06e-06 | Tokens/s: 31418\n",
            "Epoch 5/5 | Step 11750 | Loss: 2.8937 | LR: 2.02e-06 | Tokens/s: 31410\n",
            "Epoch 5/5 | Step 11760 | Loss: 3.5694 | LR: 1.97e-06 | Tokens/s: 31408\n",
            "Epoch 5/5 | Step 11770 | Loss: 2.7017 | LR: 1.93e-06 | Tokens/s: 31411\n",
            "Epoch 5/5 | Step 11780 | Loss: 3.3426 | LR: 1.89e-06 | Tokens/s: 31413\n",
            "Epoch 5/5 | Step 11790 | Loss: 3.0144 | LR: 1.85e-06 | Tokens/s: 31416\n",
            "Epoch 5/5 | Step 11800 | Loss: 2.7185 | LR: 1.81e-06 | Tokens/s: 31416\n",
            "Epoch 5/5 | Step 11810 | Loss: 3.6249 | LR: 1.77e-06 | Tokens/s: 31419\n",
            "Epoch 5/5 | Step 11820 | Loss: 3.7774 | LR: 1.73e-06 | Tokens/s: 31421\n",
            "Epoch 5/5 | Step 11830 | Loss: 3.6473 | LR: 1.69e-06 | Tokens/s: 31423\n",
            "Epoch 5/5 | Step 11840 | Loss: 3.0089 | LR: 1.65e-06 | Tokens/s: 31426\n",
            "Epoch 5/5 | Step 11850 | Loss: 3.7342 | LR: 1.62e-06 | Tokens/s: 31428\n",
            "Epoch 5/5 | Step 11860 | Loss: 3.4896 | LR: 1.58e-06 | Tokens/s: 31430\n",
            "Epoch 5/5 | Step 11870 | Loss: 2.9540 | LR: 1.55e-06 | Tokens/s: 31433\n",
            "Epoch 5/5 | Step 11880 | Loss: 2.8436 | LR: 1.52e-06 | Tokens/s: 31435\n",
            "Epoch 5/5 | Step 11890 | Loss: 2.9395 | LR: 1.48e-06 | Tokens/s: 31438\n",
            "Epoch 5/5 | Step 11900 | Loss: 2.8629 | LR: 1.45e-06 | Tokens/s: 31440\n",
            "Epoch 5/5 | Step 11910 | Loss: 2.8711 | LR: 1.42e-06 | Tokens/s: 31443\n",
            "Epoch 5/5 | Step 11920 | Loss: 3.0410 | LR: 1.39e-06 | Tokens/s: 31445\n",
            "Epoch 5/5 | Step 11930 | Loss: 2.9413 | LR: 1.37e-06 | Tokens/s: 31435\n",
            "Epoch 5/5 | Step 11940 | Loss: 3.0480 | LR: 1.34e-06 | Tokens/s: 31427\n",
            "Epoch 5/5 | Step 11950 | Loss: 3.4191 | LR: 1.31e-06 | Tokens/s: 31421\n",
            "Epoch 5/5 | Step 11960 | Loss: 2.0993 | LR: 1.29e-06 | Tokens/s: 31414\n",
            "Epoch 5/5 | Step 11970 | Loss: 2.7814 | LR: 1.27e-06 | Tokens/s: 31416\n",
            "Epoch 5/5 | Step 11980 | Loss: 3.0547 | LR: 1.24e-06 | Tokens/s: 31419\n",
            "Epoch 5/5 | Step 11990 | Loss: 3.1195 | LR: 1.22e-06 | Tokens/s: 31421\n",
            "Epoch 5/5 | Step 12000 | Loss: 2.9154 | LR: 1.20e-06 | Tokens/s: 31424\n",
            "Epoch 5/5 | Step 12010 | Loss: 3.0563 | LR: 1.18e-06 | Tokens/s: 31426\n",
            "Epoch 5/5 | Step 12020 | Loss: 4.5917 | LR: 1.16e-06 | Tokens/s: 31428\n",
            "Epoch 5/5 | Step 12030 | Loss: 2.9413 | LR: 1.15e-06 | Tokens/s: 31431\n",
            "Epoch 5/5 | Step 12040 | Loss: 2.7318 | LR: 1.13e-06 | Tokens/s: 31433\n",
            "Epoch 5/5 | Step 12050 | Loss: 3.0855 | LR: 1.11e-06 | Tokens/s: 31434\n",
            "Epoch 5/5 | Step 12060 | Loss: 4.7977 | LR: 1.10e-06 | Tokens/s: 31436\n",
            "Epoch 5/5 | Step 12070 | Loss: 4.4413 | LR: 1.09e-06 | Tokens/s: 31438\n",
            "Epoch 5/5 | Step 12080 | Loss: 4.6512 | LR: 1.07e-06 | Tokens/s: 31440\n",
            "Epoch 5/5 | Step 12090 | Loss: 4.6738 | LR: 1.06e-06 | Tokens/s: 31442\n",
            "Epoch 5/5 | Step 12100 | Loss: 4.4072 | LR: 1.05e-06 | Tokens/s: 31444\n",
            "Epoch 5/5 | Step 12110 | Loss: 4.5773 | LR: 1.04e-06 | Tokens/s: 31447\n",
            "Epoch 5/5 | Step 12120 | Loss: 4.3291 | LR: 1.03e-06 | Tokens/s: 31449\n",
            "Epoch 5/5 | Step 12130 | Loss: 4.5182 | LR: 1.02e-06 | Tokens/s: 31445\n",
            "Epoch 5/5 | Step 12140 | Loss: 4.4119 | LR: 1.02e-06 | Tokens/s: 31437\n",
            "Epoch 5/5 | Step 12150 | Loss: 4.7652 | LR: 1.01e-06 | Tokens/s: 31430\n",
            "Epoch 5/5 | Step 12160 | Loss: 4.5215 | LR: 1.01e-06 | Tokens/s: 31420\n",
            "Epoch 5/5 | Step 12170 | Loss: 4.2926 | LR: 1.00e-06 | Tokens/s: 31421\n",
            "Epoch 5/5 | Step 12180 | Loss: 4.2222 | LR: 1.00e-06 | Tokens/s: 31422\n",
            "Epoch 5/5 | Step 12190 | Loss: 4.6370 | LR: 1.00e-06 | Tokens/s: 31425\n",
            "Epoch 5/5 | Step 12200 | Loss: 4.4684 | LR: 1.00e-06 | Tokens/s: 31427\n",
            "\n",
            "========================================\n",
            "Epoch 5 complete | Average Loss: 3.0441\n",
            "========================================\n",
            "\n",
            "Training complete! Total time: 795.0s\n",
            "\n",
            "8. Saving model...\n",
            "Model saved to model.npz\n",
            "Config saved to model_config.json\n",
            "Tokenizer saved to tokenizer.json\n",
            "\n",
            "============================================================\n",
            "[OK] Training complete!\n",
            "   Model saved to: model.npz\n",
            "   Tokenizer saved to: tokenizer.json\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 4. Run Training\n",
        "# - Resumes from best_model.npz (if it exists)\n",
        "# - Saves to best_model_new.npz\n",
        "# - Creates a NEW tokenizer file\n",
        "\n",
        "# Protect original tokenizer\n",
        "!cp tokenizer.json tokenizer_new.json 2>/dev/null || echo \"No tokenizer.json found, will build new one.\"\n",
        "\n",
        "!python cli.py train \\\n",
        "    --device gpu \\\n",
        "    --load-model best_model.npz \\\n",
        "    --epochs 5 \\\n",
        "    --batch-size 32 \\\n",
        "    --log-interval 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python cli.py train --epochs 5 --batch-size 64  --grad-accum 4 --vocab-size 7171  --chat-only  --device gpu  --load-model best_model.npz"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cZTInPmpAWe",
        "outputId": "23bf1eed-e215-4896-f046-480af997c775"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backend: GPU (Unknown GPU, 15095MB VRAM)\n",
            "============================================================\n",
            ">> Training PyCodeAI\n",
            "Device: GPU\n",
            "============================================================\n",
            "\n",
            "1. Loading training data...\n",
            "    Chat Mode: Loading dedicated conversation data\n",
            "[OK] Loaded 103 articles\n",
            "[OK] Loaded 3153 chat/article samples\n",
            "   Loaded 3153 samples\n",
            "\n",
            "2. Building tokenizer...\n",
            "   Loading tokenizer from tokenizer.json...\n",
            "Tokenizer loaded from tokenizer.json\n",
            "   Vocabulary size: 7171\n",
            "\n",
            "3. Tokenizing data...\n",
            "   Tokenized 3153 samples\n",
            "\n",
            "4. Creating model...\n",
            "   Loading weights from best_model.npz...\n",
            "Model loaded from best_model.npz\n",
            "   [OK] Weights loaded successfully!\n",
            "   Model parameters: 2,629,120\n",
            "GPTConfig(\n",
            "  vocab_size=7171,\n",
            "  max_seq_len=64,\n",
            "  embed_dim=128,\n",
            "  num_heads=4,\n",
            "  num_layers=4,\n",
            "  expansion_factor=4\n",
            ")\n",
            "\n",
            "5. Creating data loader...\n",
            "   Batches per epoch: 371\n",
            "\n",
            "6. Setting up trainer...\n",
            "\n",
            "7. Starting training...\n",
            "============================================================\n",
            "Starting Training\n",
            "Model parameters: 2,629,120\n",
            "Epochs: 5\n",
            "Batches per epoch: 371\n",
            "Gradient Accumulation: 4 steps\n",
            "Effective Batch Size: 256\n",
            "============================================================\n",
            "Epoch 1/5 | Step 10 | Loss: 4.1370 | LR: 3.00e-05 | Tokens/s: 65777\n",
            "Epoch 1/5 | Step 20 | Loss: 3.7674 | LR: 6.00e-05 | Tokens/s: 70781\n",
            "Epoch 1/5 | Step 30 | Loss: 3.9569 | LR: 9.00e-05 | Tokens/s: 68249\n",
            "Epoch 1/5 | Step 40 | Loss: 4.0864 | LR: 1.20e-04 | Tokens/s: 71629\n",
            "Epoch 1/5 | Step 50 | Loss: 3.7982 | LR: 1.50e-04 | Tokens/s: 73708\n",
            "Epoch 1/5 | Step 60 | Loss: 3.8182 | LR: 1.80e-04 | Tokens/s: 75052\n",
            "Epoch 1/5 | Step 70 | Loss: 3.8320 | LR: 2.10e-04 | Tokens/s: 76192\n",
            "Epoch 1/5 | Step 80 | Loss: 3.7460 | LR: 2.40e-04 | Tokens/s: 76842\n",
            "Epoch 1/5 | Step 90 | Loss: 3.7434 | LR: 2.70e-04 | Tokens/s: 76058\n",
            "\n",
            "========================================\n",
            "Epoch 1 complete | Average Loss: 3.8712\n",
            "========================================\n",
            "\n",
            "Model saved to best_model.npz\n",
            "Config saved to best_model_config.json\n",
            "Checkpoint saved: best_model.npz\n",
            "Epoch 2/5 | Step 103 | Loss: 3.9588 | LR: 3.00e-04 | Tokens/s: 76140\n",
            "Epoch 2/5 | Step 113 | Loss: 3.7332 | LR: 3.00e-04 | Tokens/s: 76735\n",
            "Epoch 2/5 | Step 123 | Loss: 3.9109 | LR: 3.00e-04 | Tokens/s: 77128\n",
            "Epoch 2/5 | Step 133 | Loss: 4.0556 | LR: 3.00e-04 | Tokens/s: 77532\n",
            "Epoch 2/5 | Step 143 | Loss: 3.7731 | LR: 3.00e-04 | Tokens/s: 77915\n",
            "Epoch 2/5 | Step 153 | Loss: 3.7445 | LR: 2.99e-04 | Tokens/s: 77135\n",
            "Epoch 2/5 | Step 163 | Loss: 3.7882 | LR: 2.99e-04 | Tokens/s: 77021\n",
            "Epoch 2/5 | Step 173 | Loss: 3.6302 | LR: 2.99e-04 | Tokens/s: 77336\n",
            "Epoch 2/5 | Step 183 | Loss: 3.7370 | LR: 2.98e-04 | Tokens/s: 77570\n",
            "\n",
            "========================================\n",
            "Epoch 2 complete | Average Loss: 3.8097\n",
            "========================================\n",
            "\n",
            "Epoch 3/5 | Step 196 | Loss: 3.9421 | LR: 2.98e-04 | Tokens/s: 77887\n",
            "Epoch 3/5 | Step 206 | Loss: 3.7233 | LR: 2.97e-04 | Tokens/s: 78090\n",
            "Epoch 3/5 | Step 216 | Loss: 3.8918 | LR: 2.97e-04 | Tokens/s: 77592\n",
            "Epoch 3/5 | Step 226 | Loss: 4.0312 | LR: 2.96e-04 | Tokens/s: 77412\n",
            "Epoch 3/5 | Step 236 | Loss: 3.8036 | LR: 2.96e-04 | Tokens/s: 77619\n",
            "Epoch 3/5 | Step 246 | Loss: 3.7294 | LR: 2.95e-04 | Tokens/s: 77761\n",
            "Epoch 3/5 | Step 256 | Loss: 3.7214 | LR: 2.94e-04 | Tokens/s: 77856\n",
            "Epoch 3/5 | Step 266 | Loss: 3.6695 | LR: 2.93e-04 | Tokens/s: 77966\n",
            "Epoch 3/5 | Step 276 | Loss: 3.7613 | LR: 2.93e-04 | Tokens/s: 77691\n",
            "\n",
            "========================================\n",
            "Epoch 3 complete | Average Loss: 3.8024\n",
            "========================================\n",
            "\n",
            "Epoch 4/5 | Step 289 | Loss: 3.9429 | LR: 2.92e-04 | Tokens/s: 77349\n",
            "Epoch 4/5 | Step 299 | Loss: 3.7220 | LR: 2.91e-04 | Tokens/s: 77332\n",
            "Epoch 4/5 | Step 309 | Loss: 3.9137 | LR: 2.90e-04 | Tokens/s: 77418\n",
            "Epoch 4/5 | Step 319 | Loss: 4.0502 | LR: 2.89e-04 | Tokens/s: 77533\n",
            "Epoch 4/5 | Step 329 | Loss: 3.8198 | LR: 2.88e-04 | Tokens/s: 77643\n",
            "Epoch 4/5 | Step 339 | Loss: 3.7116 | LR: 2.87e-04 | Tokens/s: 77362\n",
            "Epoch 4/5 | Step 349 | Loss: 3.7212 | LR: 2.85e-04 | Tokens/s: 77262\n",
            "Epoch 4/5 | Step 359 | Loss: 3.6108 | LR: 2.84e-04 | Tokens/s: 77342\n",
            "Epoch 4/5 | Step 369 | Loss: 3.7366 | LR: 2.83e-04 | Tokens/s: 77415\n",
            "\n",
            "========================================\n",
            "Epoch 4 complete | Average Loss: 3.8134\n",
            "========================================\n",
            "\n",
            "Epoch 5/5 | Step 382 | Loss: 3.9261 | LR: 2.81e-04 | Tokens/s: 77459\n",
            "Epoch 5/5 | Step 392 | Loss: 3.7010 | LR: 2.80e-04 | Tokens/s: 77519\n",
            "Epoch 5/5 | Step 402 | Loss: 3.9106 | LR: 2.79e-04 | Tokens/s: 77120\n",
            "Epoch 5/5 | Step 412 | Loss: 3.9988 | LR: 2.77e-04 | Tokens/s: 76975\n",
            "Epoch 5/5 | Step 422 | Loss: 3.7837 | LR: 2.76e-04 | Tokens/s: 77077\n",
            "Epoch 5/5 | Step 432 | Loss: 3.6931 | LR: 2.74e-04 | Tokens/s: 77144\n",
            "Epoch 5/5 | Step 442 | Loss: 3.6844 | LR: 2.73e-04 | Tokens/s: 77174\n",
            "Epoch 5/5 | Step 452 | Loss: 3.5772 | LR: 2.71e-04 | Tokens/s: 77225\n",
            "Epoch 5/5 | Step 462 | Loss: 3.7249 | LR: 2.70e-04 | Tokens/s: 76988\n",
            "\n",
            "========================================\n",
            "Epoch 5 complete | Average Loss: 3.7805\n",
            "========================================\n",
            "\n",
            "Training complete! Total time: 98.9s\n",
            "\n",
            "8. Saving model...\n",
            "Model saved to model.npz\n",
            "Config saved to model_config.json\n",
            "Tokenizer saved to tokenizer.json\n",
            "\n",
            "============================================================\n",
            "[OK] Training complete!\n",
            "   Model saved to: model.npz\n",
            "   Tokenizer saved to: tokenizer.json\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIs0wNzqikAe",
        "outputId": "d8aec767-e726-4cab-f47a-74ae7a381162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing up to Drive as best_model_20260210_175011.npz...\n",
            "Model saved.\n",
            "Tokenizer saved.\n"
          ]
        }
      ],
      "source": [
        "# 5. Save Results to Drive\n",
        "import shutil\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "new_model_name = f\"best_model_{timestamp}.npz\"\n",
        "new_token_name = f\"tokenizer_{timestamp}.json\"\n",
        "\n",
        "print(f\"Backing up to Drive as {new_model_name}...\")\n",
        "\n",
        "# Copy model\n",
        "if os.path.exists(\"best_model.npz\"): # Changed from best_model_new.npz\n",
        "    shutil.copy(\"best_model.npz\", os.path.join(DRIVE_SAVE_PATH, new_model_name)) # Changed from best_model_new.npz\n",
        "    # Also update the 'latest' one\n",
        "    shutil.copy(\"best_model.npz\", os.path.join(DRIVE_SAVE_PATH, \"best_model_latest.npz\")) # Changed from best_model_new.npz\n",
        "    print(\"Model saved.\")\n",
        "else:\n",
        "    print(\"ERROR: best_model.npz not found! (Was looking for best_model_new.npz before)\")\n",
        "\n",
        "# Copy tokenizer\n",
        "if os.path.exists(\"tokenizer_new.json\"):\n",
        "    shutil.copy(\"tokenizer_new.json\", os.path.join(DRIVE_SAVE_PATH, new_token_name))\n",
        "    shutil.copy(\"tokenizer_new.json\", os.path.join(DRIVE_SAVE_PATH, \"tokenizer_latest.json\"))\n",
        "    print(\"Tokenizer saved.\")\n",
        "else:\n",
        "    print(\"WARNING: tokenizer_new.json not found!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"## Git Push Process\")\n",
        "print(\"\\n1.  **Stage changes**: Add files to the staging area.\")\n",
        "print(\"!git add .  # To add all changes in the current directory\")\n",
        "print(\"# Or: !git add <file_name> # To add a specific file\")\n",
        "print(\"\\n2.  **Commit changes**: Record the changes to the repository with a message.\")\n",
        "print(\"!git commit -m \\\"Your commit message here\\\"\")\n",
        "print(\"\\n3.  **Push to remote**: Upload the committed changes to the remote repository.\")\n",
        "print(\"!git push origin main # Replace 'main' with your branch name (e.g., master)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wczWugveq41w",
        "outputId": "21a42cab-b271-4522-ebcc-6d067a7b4a9d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Git Push Process\n",
            "\n",
            "1.  **Stage changes**: Add files to the staging area.\n",
            "!git add .  # To add all changes in the current directory\n",
            "# Or: !git add <file_name> # To add a specific file\n",
            "\n",
            "2.  **Commit changes**: Record the changes to the repository with a message.\n",
            "!git commit -m \"Your commit message here\"\n",
            "\n",
            "3.  **Push to remote**: Upload the committed changes to the remote repository.\n",
            "!git push origin main # Replace 'main' with your branch name (e.g., master)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}